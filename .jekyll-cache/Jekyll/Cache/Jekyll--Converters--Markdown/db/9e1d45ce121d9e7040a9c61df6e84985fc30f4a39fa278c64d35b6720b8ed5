I"¤¢<ul>
  <li><a href="#mulu">æ¬¢è¿æ¥åˆ°çˆ¬è™«ç¬”è®°é¡µé¢</a></li>
</ul>

<!-- /code_chunk_output -->
<p>&lt;!DOCTYPE html&gt;
#<center>&lt;h1&gt;çˆ¬è™«&lt;/h1&gt;</center>
ç‚¹å‡»è¿™é‡Œè¿˜å¯ä»¥å»<a href="https://mrsyangjie.github.io/" title="æˆ‘çš„åšå®¢ä¸»é¡µ">æˆ‘çš„åšå®¢ä¸»é¡µ</a>o 
ç‚¹å‡»è¿™é‡Œè¿˜å¯ä»¥å»<a href="https://www.cnblogs.com/zhaof/tag/%E7%88%AC%E8%99%AB/default.html?page=2" title="å­¦ä¹ ç¬”è®°">å¤§ç¥çš„å­¦ä¹ ç¬”è®°</a>o</p>
<font face="å®‹ä½“"> <span id="qianyan"><center>å‰è¨€</center></span></font>
<center><p>æœ¬é¡µåªæ˜¯ä¸ºäº†æ–¹ä¾¿æœ¬äººä»¥åå¤ä¹ çˆ¬è™«ç”¨çš„ç¬”è®°markdown</p></center>

<font color="#1c9999">çº¯å±å¨±ä¹ï¼Œå¦‚æœ‰é›·åŒï¼Œæ‰“æ­»ä¸è®¤*â€”â€”*</font>

<center>
&lt;img 
src='http://images2017.cnblogs.com/blog/764024/201802/764024-20180205162555654-1350503259.jpg'
width=500  / &gt;
</center>

<hr />

<ol>
  <li><a href="#1">ä»€ä¹ˆæ˜¯ç½‘ç»œçˆ¬è™«ä¸çˆ¬è™«å®ç°åŸç†</a></li>
  <li><a href="#2">Urllibç®€å•çš„çˆ¬å–ç½‘é¡µ</a></li>
  <li><a href="#3">ç¼–ç é—®é¢˜</a></li>
  <li><a href="#4">requestsåº“</a></li>
  <li><a href="#5">æ­£åˆ™è¡¨è¾¾å¼å’ŒBeautifulSoup</a></li>
  <li><a href="#6">selenium</a></li>
  <li><a href="#7">cookie</a></li>
  <li><a href="#8">mongoDB</a></li>
  <li><a href="#9">pyç”Ÿæˆexe</a></li>
  <li><a href="#10">è‡ªå·±çš„æ¡ˆåˆ—</a>
    <hr />
  </li>
</ol>
<font face="å®‹ä½“"> <span id="1" color="#456526"><center>ä»€ä¹ˆæ˜¯ç½‘ç»œçˆ¬è™«ä¸çˆ¬è™«å®ç°åŸç†</center></span></font>
<ul>
  <li>ç½‘ç»œçˆ¬è™«ç”± æ§åˆ¶èŠ‚ç‚¹ï¼Œçˆ¬è™«èŠ‚ç‚¹ï¼Œèµ„æºåº“æ„æˆï¼Œè€Œåœ¨å†™çˆ¬è™«ä¸­ä¸€å®šæ³¨æ„å…ƒç»„ï¼ˆï¼‰ä¸åˆ—è¡¨çš„åŒºåˆ«[]ï¼Œå­—å…¸{}å’Œé›†åˆ{}çš„åŒºåˆ«
çˆ¬è™«çš„æµç¨‹
æ‰‹åŠ¨æ“ä½œ
åˆ†æé¡µé¢æ•°æ®
1æŸ¥çœ‹æºä»£ç 
2æŸ¥çœ‹æ˜¯å¦æ˜¯ a jaxå¼‚æ­¥åŠ è½½
3è§£å¯† ï¼ˆéå¸¸ç†Ÿæ‚‰å‰ç«¯ï¼Œç‰¹åˆ«æ˜¯jsï¼‰
decode  è§£ç  æŠŠå…¶ä»–ç¼–ç è½¬åŒ–ä¸ºunicode decode(â€˜utf8â€™)æŠŠutf-8è½¬åŒ–ä¸ºunicode
encode  ç¼–ç   æŠŠunicodè½¬åŒ–ä¸ºå…¶ä»–ç¼–ç   encode(â€˜gbkâ€™) æŠŠunicodeè½¬åŒ–ä¸ºgbk
å¯ä»¥è¿™æ ·ç†è§£unicodeä¸ºä¸­æ–‡ gbkä¸ºæ—¥æ–‡  utf-8ä¸ºè‹±æ–‡
è½¬ç åªèƒ½è½¬ä¸­æ–‡ æ‰€ä»¥å°±æ˜¯   æ—¥æ–‡è½¬è‹±æ–‡ =å°±æ˜¯ æ—¥è½¬ä¸­è½¬è‹±  unicodeå°±æ˜¯åƒä¸­é—´äºº
æ‰€ä»¥gbkæ€æ ·è½¬ä¸ºutf-8 ï¼š decode(â€˜gbkâ€™).encode(â€˜utf8â€™)
åœ¨è¿™ç»™è‡ªå·±è¡¥å……ä¸‹ï¼Œæˆ‘ç°åœ¨æ˜¯ä¸»è¦ç”¨pycharmå’Œvscodeå’Œjupytur notebookå†™python
pycharmçš„è¯ï¼Œæ“ä½œç®€å•ï¼Œå†™èµ·æ¥æ–¹ä¾¿å¿«æ·ï¼Œä½†æ˜¯æ³¨æ„è™šæ‹Ÿç¯å¢ƒï¼Œè¿™ä¸ªå¯ä»¥å‚è€ƒDjango.mdï¼Œè€Œä¸”åœ¨pipæ—¶ç®€å•ï¼Œå¦‚æœé”™è¯¯å°±æ˜¯åŸå› æ˜¯è¿æ¥è¶…æ—¶ï¼Œæ‰€ä»¥éœ€è¦è‡ªå·±è®¾å®šå®‰è£…æºï¼Œ
è§£å†³æ–¹æ³•ï¼š
åœ¨ pipå‘½ä»¤åè‡ªå·±è®¾å®šæ”¶é›†æºï¼ˆ-i +urlï¼‰
egï¼špip install requests -i http://pypi.douban.com/simple â€“trusted-host pypi.douban.comï¼ˆé€šè¿‡è±†ç“£ï¼‰å°±okäº†
ç”±äºè‡ªå·±æ‰‹æ®‹ã€‚æŠŠè™šæ‹Ÿåˆ é™¤äº†ï¼Œæ‰€ä»¥é‡æ–°virtualenv pachong_envè¿˜è¦åœ¨pycharmé‡Œé¢é€‰æ‹©å·²ç»å­˜åœ¨çš„è™šæ‹Ÿå’Œè§£é‡Šå™¨ï¼Œé€‰æ‹©æ˜¯å·²ç»å­˜åœ¨çš„ï¼Œä¸ç„¶é‡æ–°çš„åˆè¦pycharmè‡ªå·±ä¸»åŠ¨åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒ</li>
</ul>

<p>vscodeçš„è¯è½»ä¾¿ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œæ¯”å¦‚æˆ‘çš„å°±æ˜¯D:\æ–°å»ºæ–‡ä»¶å¤¹ï¼Œä¸ºä»€ä¹ˆåœ¨è¿™ä¸ªæ–‡ä»¶å¤¹é‡Œé¢èƒ½å†™pythonå› ä¸ºå·²ç»åœ¨vscodeé‡Œè®¾ç½®äº†è¯¦æƒ…è‡ªå·±ç™¾åº¦ï¼Œè€Œä¸”é‡Œé¢æœ‰ä¸€ä¸ªD:\æ–°å»ºæ–‡ä»¶å¤¹.vscode\setting.josnï¼Œè¿™å°±æ˜¯å…³é”®
jupytur notebook ç›´æ¥æ‰“å¼€annocodä½†æ˜¯ä¹Ÿå¯ä»¥ç›´æ¥æ‰“å¼€ç¬”è®°æœ¬é€šè¿‡æµè§ˆå™¨127.0.0.1:8888ä¹Ÿå¯ä»¥</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>å…ƒç»„æ˜¯ä¸€ç§åºåˆ—ç±»å‹ï¼Œå¯ä»¥ç´¢å¼•ï¼Œè€Œä¸”å¯ä»¥ç”¨ï¼Œå’Œï¼ˆï¼‰å®šä¹‰ï¼Œå½“æ—¶å…ƒç»„æ˜¯ä¸å¯å˜çš„åˆ—è¡¨æ˜¯å¯å˜çš„
åˆ—è¡¨æ˜¯ä¸€ç§å¸¸è§çš„æ•°æ®ç»“æ„ï¼Œä¸å…ƒç»„ç›¸æ¯”ï¼Œåˆ—è¡¨æ˜¯å¯å˜çš„ï¼Œlenå¯ä»¥è·å¾—åˆ—è¡¨çš„é•¿åº¦ï¼Œåˆ—è¡¨æ”¯æŒç´¢å¼•å’Œåˆ‡ç‰‡ï¼Œè¿˜å¯ä»¥appendåœ¨æœ«å°¾è¿½åŠ æ–°çš„å…ƒç´ å’Œextendåœ¨æœ«å°¾è¿½åŠ æ–°çš„åºåˆ—ï¼ˆå¦‚å…ƒç»„æˆ–åˆ—è¡¨ï¼‰æ–¹æ³•ï¼Œè¿˜æ”¯æŒpop,insert,remove,
å­—å…¸æ˜¯ä¸€ç§å¸¸ç”¨çš„æ•°æ®ç»“æ„ï¼Œå­—å…¸æ‰€ä»¥ä¸åŒï¼Œä½¿ç”¨ï¼ˆkeyï¼‰é”®è¿›è¡Œç´¢å¼•ï¼Œä¸é”®å¯¹åº”çš„å€¼æ˜¯å€¼(value)ï¼Œkeyså°±æ˜¯é”®å’Œå€¼ï¼Œå­—å…¸å°±æ˜¯ç”±è‹¥å¹²é”®å€¼å¯¹æ„æˆï¼Œå¹¶ä¸”é”®å€¼éƒ½æœ‰å¼•å·ï¼Œå†’å·åˆ†å¼€
header={'host':www.adad .com'}
é›†åˆçš„ç‰¹ç‚¹æ˜¯æ— åºè€Œä¸”æ— é‡å¤å…ƒç´ ï¼Œå¯ä»¥ä½¿ç”¨set()å’Œå¤§æ‹¬å·{}æ¥åˆå§‹åŒ–é›†åˆ
s=set(['a','b'])
s={'a','b'}
enumerateæ˜¯ä¸€ç§æ“ä½œå‡½æ•°å¯ä»¥è¿”å›åˆ—è¡¨å…ƒç»„çš„ç´¢å¼•å’Œå€¼
for ind ,val in enumerate([10,20,40])
print(ind,val)
...
0 10
1 20
2 40
...
åœ¨å¾ˆé•¿çš„ä»£ç ä¸­å¾ˆæœ‰å¯èƒ½å‡ºé—®é¢˜ï¼Œæ‰€ä»¥æœ‰å¼‚å¸¸å¤„ç†æœºåˆ¶
try:
    print('to do')
    a = 10/0
    print(a)
except Exception as e:
    print(e)
finally:
    print('æˆ‘éƒ½æ‰§è¡Œ')
    æ‰§è¡Œçš„ç»“æœä¸º
    ---
to do
division by zero
æˆ‘éƒ½æ‰§è¡Œ
------
#é‚£æ€æ ·ä¸»åŠ¨æŠ›å‡ºä¸€ä¸ªé”™è¯¯
æ¥æ¥ç€çœ‹ï¼š
class makeErr(ValueError):
    pass


def make_error(a):
    n = int(a)
    if n==0:
        raise makeErr('æˆ‘æ˜¯ä¸€ä¸ªé”™è¯¯')
    return 1/n


make_error(0)
ç»“æœä¸º
    make_error(0)
  File "D:/Python_code/pachong/pachong1.py", line 18, in make_error
    raise makeErr('æˆ‘æ˜¯ä¸€ä¸ªé”™è¯¯')
__main__.makeErr: æˆ‘æ˜¯ä¸€ä¸ªé”™è¯¯
</code></pre></div></div>
<p>çˆ¬è™«æŒ‰ç…§æŠ€æœ¯å’Œç»“æ„åˆ†ä¸ºé€šç”¨ç½‘ç»œçˆ¬è™«ï¼Œèšç„¦ç½‘ç»œçˆ¬è™«ï¼Œå¢é‡å¼ç½‘ç»œçˆ¬è™«ï¼Œæ·±å±‚ç½‘ç»œçˆ¬è™«ç­‰ï¼Œå®é™…ç½‘ç»œçˆ¬è™«é€šå¸¸æ˜¯å‡ ç§çš„ç»„åˆä½“
ä¸€èˆ¬éƒ½æ˜¯ç”±urlé›†åˆï¼Œurlé˜Ÿåˆ—ï¼Œé¡µé¢çˆ¬å–ï¼Œé¡µé¢åˆ†æï¼Œé¡µé¢æ•°æ®åº“ï¼Œé“¾æ¥è¿‡æ»¤ï¼Œå†…å®¹è¯„ä»·ï¼Œé“¾æ¥è¯„ä»·ï¼Œæƒ³åŠæ³•è‡ªåŠ¨å¡«å†™è¡¨å•ç­‰</p>
<ul>
  <li>æˆ‘ä»¬é€šè¿‡ä¸¤ç§ç»å…¸çˆ¬è™«ä¸ºä¾‹ï¼ˆé€šç”¨çˆ¬è™«å’Œèšç„¦çˆ¬è™«ä¸ºä¾‹ï¼‰åˆ†åˆ«è®°å½•ä»–ä»¬çš„å®ç°åŸç†å¹¶ä¸”é€šè¿‡ä»–ä»¬æ‰¾åˆ°å…±åŒç‚¹
    <ol>
      <li>é€šç”¨ç½‘ç»œçˆ¬è™«
1è·å–åˆå§‹çš„urlï¼Œåˆå§‹çš„urlåœ°å€å¯ä»¥ ç”±ç”¨æˆ·äººä¸ºçš„æŒ‡å®šï¼Œä¹Ÿå¯ä»¥ç”±ç”¨æˆ·æŒ‡å®šçš„æŸä¸ªæˆ–å‡ ä¸ªåˆå§‹çš„ç½‘é¡µå†³å®š
2æ ¹æ®åˆå§‹çš„URLçˆ¬å–é¡µé¢å¹¶è·å¾—æ–°çš„URLï¼Œå°†ç½‘é¡µå‚¨è“„åœ¨åŸå§‹çš„æ•°æ®åº“ä¸­ï¼Œå¹¶ä¸”åœ¨çˆ¬å–ç½‘é¡µä¸­å‘ç°äº†æ–°çš„urlå°†å·²çˆ¬å–çš„URLåœ°å€å­˜æ”¾åœ¨ä¸€ä¸ªURLåˆ—è¡¨ä¸­ï¼Œå†å»é‡æˆ–è€…åˆ¤æ–­çˆ¬å–çš„è¿›ç¨‹
3çˆ¬å–æ–°çš„URLåˆ—è¡¨é‡Œçš„URLå¹¶ä¸”æœ‰å¯èƒ½å†çˆ¬å–æ–°URLä¸­åˆå†è·å¾—URLå†åŠ å…¥åˆ—è¡¨ï¼Œä»¥æ­¤é‡å¤
4æ»¡è¶³çˆ¬è™«ç³»ç»Ÿçš„åœæ­¢æ¡ä»¶ååœæ­¢çˆ¬å–</li>
      <li>èšç„¦ç½‘ç»œçˆ¬è™«
1ç”±äºå…¶æœ‰ç›®çš„çš„çˆ¬å–ï¼Œæ‰€ä»¥å¯¹äºèšç„¦çˆ¬è™«æ¥è¯´å¿…é¡»å¢åŠ ç›®æ ‡çš„å®šä¹‰å’Œè¿‡æ»¤æœºåˆ¶è¿˜æœ‰ä¸‹ä¸€å¥URLåœ°å€çš„é€‰å–ç­‰ï¼Œæ‰€ä»¥ç¬¬ä¸€æ­¥æ˜¯åšå¥½çˆ¬å–éœ€æ±‚çš„å®šä¹‰å’Œæè¿°
2è·å–åˆå§‹URL
3æ ¹æ®åˆå§‹URLçˆ¬å–å¹¶è·å¾—æ–°çš„URL
4ä»æ–°çš„URLè¿‡æ»¤æ‰æ— ç”¨çš„ï¼Œå¯¹æœ‰éœ€è¦çš„æ”¾å…¥URLåˆ—è¡¨ï¼Œå†å»é‡å’Œåˆ¤æ–­çˆ¬å–çš„è¿›ç¨‹
5ä»URLåˆ—è¡¨ä¸­ï¼Œæ ¹æ®æœç´¢ç®—æ³•ï¼Œç¡®å®šURLä¼˜å…ˆçº§ï¼Œå¹¶ç¡®å®šä¸‹ä¸€æ­¥è¦çˆ¬å–çš„URLåœ°å€ï¼Œåœ¨èšç„¦çˆ¬è™«ä¸­ï¼Œç¡®å®šä¸‹ä¸€æ­¥çš„URLæ¯”è¾ƒé‡è¦
ç„¶ååˆæ˜¯ä»ä¸‹ä¸€æ­¥çš„URLè¯»å–æ–°çš„URLï¼Œå¹¶é‡å¤
6æ»¡è¶³åœæ­¢æ¡ä»¶æ—¶åœæ­¢</li>
    </ol>
  </li>
</ul>

<hr />
<font face="å®‹ä½“"> <span id="2" color="#456526"><center>é€šè¿‡Urllibç®€å•çš„çˆ¬å–ç½‘é¡µ</center></span></font>

<hr />

<ul>
  <li>ä½¿ç”¨urllibåº“çˆ¬å–ç½‘é¡µçš„ç®€å•æ“ä½œ
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>åœ¨ä½¿ç”¨urllibå‰çŸ¥é“ï¼Œå½“ä½ è·å–ä¸€ä¸ªURLä½ ä½¿ç”¨ä¸€ä¸ªopenerã€‚åœ¨ä¸€èˆ¬ï¼Œæˆ‘ä»¬éƒ½æ˜¯ä½¿ç”¨çš„é»˜è®¤çš„openerï¼Œä¹Ÿå°±æ˜¯urlopenã€‚å®ƒæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„openerï¼Œå¯ä»¥ç†è§£æˆopenerçš„ä¸€ä¸ªç‰¹æ®Šå®ä¾‹ï¼Œä¼ å…¥çš„å‚æ•°ä»…ä»…æ˜¯urlï¼Œdataï¼Œtimeoutã€‚
import urllib.request
url='http://www.baidu.com'
print('ç¬¬ä¸€ç§')
response1=urllib.request.urlopen(url)
print (response1.getcode())
print (len(response1.read()))
ç»“æœä¸º
ç¬¬ä¸€ç§
200
153295
</code></pre></div>    </div>
    <p>urllibæ€è·¯ä¹Ÿæ˜¯å¤§æ¦‚ä¸€æ ·
1é¦–å…ˆï¼Œçˆ¬å–ä¸€ä¸ªç½‘é¡µå¹¶è¯»å‡ºæ¥èµ‹ç»™ä¸€ä¸ªå˜é‡
2 ä»¥å†™å…¥çš„æ–¹å¼æ‰“å¼€ä¸€ä¸ªæ–‡ä»¶ï¼Œå°†å€¼å†™å…¥æ–‡ä»¶
3å…³é—­æ–‡ä»¶</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import urllib.request
file = urllib.request.urlopen('http://www.baidu.com')
data=file.read()
fhandle=open('D:/Python_code/pachong/handle/1.html','wb')
fhandle.write(data)
fhandle.close()
ä¹‹å‰ä¸€ç›´é”™å› ä¸ºæˆ‘å†™é”™äº†
fhandle=open('D:D:\Python_code\pachong\handle\1.html','wb')
åƒä¸‡åˆ«å¤åˆ¶è·¯å¾„ï¼Œå¯èƒ½æˆ‘çš„ç”µè„‘è¿™åªåŸå› ï¼Œæ–œæ åçš„ ä¸€ç›´é”™è¯¯
</code></pre></div>    </div>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>å¦‚æœè¦è¿›è¡Œç¼–ç å’Œè§£ç ï¼Œæ¯”å¦‚æˆ‘ä»¬å¯¹ç½‘å€è¿›è¡Œç¼–ç 
import urllib.request
file = urllib.request.quote('http://www.baidu.com')
print(file)
quote=urllib.request.unquote('http%3A//www.baidu.com')
print(quote)
è¾“å‡ºä¸º
D:\Python_code\env_pacong\Scripts\python.exe D:/Python_code/pachong/pachong1.py
http%3A//www.baidu.com
http://www.baidu.com
ä¸‹é¢å†æ¥çœ‹ä¸€ä¸ªå…¸å‹çš„æˆ‘çš„é”™è¯¯
import urllib.request
url = 'http://www.baidu.com'
file=urllib.request.urlopen('url')
print(file.text)
é”™è¯¯ä¸€urlopené‡Œé¢æ˜¯å‚æ•°æ—¶ä¸ç”¨å¼•å·
é”™è¯¯äºŒurlopené‡Œæ²¡æœ‰textå‚æ•°åªæœ‰requestsæ‰æœ‰ï¼Œåˆ«ææ··äº†ï¼Œurllibåªæœ‰é€šè¿‡ä¸Šé¢çš„read()æ‰èƒ½è¯»è€Œ
import requests
url = 'http://www.baidu.com'
file=requests.get(url)
print(file.text)
requestè¿™æ ·å°±å¯ä»¥è¯»
</code></pre></div>    </div>
    <p>å¥½äº†ï¼Œå¤§æ¦‚çš„urllibæµç¨‹ä¹Ÿä¼šäº†ï¼Œç°åœ¨æ¥ç‚¹ä¸ä¸€æ ·çš„å§
æµè§ˆå™¨çš„æ¨¡æ‹Ÿâ€“Heatherså±æ€§
å› ä¸ºä¸€äº›ç½‘ç«™åŸå› ä¸èƒ½çˆ¬å–ï¼Œè¿™æ—¶éœ€è¦æˆ‘ä»¬æ¨¡æ‹Ÿç½‘ç«™è®¿é—®
ç¬¬ä¸€æ­¥æ‰“å¼€ä½ æƒ³è¦çš„ç½‘ç«™ç‚¹è¿›å»äº‹ä»¶å‘ç”Ÿï¼Œç„¶åæ£€æŸ¥æˆ–è€…f12æ‰¾åˆ°user-AgentæŠŠå®ƒå¤åˆ¶ä¸‹æ¥æ¯”å¦‚bingçš„å°±æ˜¯
Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import urllib.request
url ='http://blog.csdn.net/weiwei_pig/article/details/51178226'
req=urllib.request.Request(url)
req.add_header('user-agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36')
data=urllib.request.urlopen(req).read()
fhandle=open('D:/Python_code/pachong/handle/3.html','wb')
fhandle.write(data)
fhandle.close()
ç„¶åæ‰“å¼€3.htmlå°±æœ‰äº†
æ–¹å¼äºŒ
import urllib.request
url='http://blog.csdn.net/weiwei_pig/article/details/51178226'
headers=('user-agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36')
opener=urllib.request.build_opener()
opener.addheaders=[headers]
data=opener.open(url).read()
fhandle=open('D:/Python_code/pachong/handle/4.html','wb')
fhandle.write(data)
fhandle.close()
ä¸€æ ·æœ‰äº†
</code></pre></div>    </div>
    <p>-ç®€å•çš„è¶…æ—¶è®¾ç½®
file = urllib.request.urlopen(â€˜http://yum.iqianyue.comâ€™,timeout=1)</p>
  </li>
</ul>

<p>-httpåè®®è¯·æ±‚
1.GETè¯·æ±‚
2.POSTè¯·æ±‚
3.PUTè¯·æ±‚
4.DELETEè¯·æ±‚
5.HEADè¯·æ±‚</p>
<ul>
  <li>GETè¯·æ±‚å®ä¾‹åˆ†æ
keyworld = â€˜helloâ€™
url = â€˜http://www.baidu.com/s?wd=â€™ + keyworld
æˆ–è€…
kv={â€˜wdâ€™:â€™pythonâ€™}
r=urllib.request.urlopen(â€˜http://www.baidu.com/sâ€™,params=kv)
å¦‚æœkeyæ˜¯ä¸­æ–‡ã€‚ç¼–ç å¯èƒ½å¸¦æ¥é—®é¢˜ï¼Œåˆ™éœ€è¦è§£ç 
import urllib.request
url=â€™http://www.baidu.com/s?wd=â€™
key=â€™é˜³â€™
key_code=urllib.request.quote(key)
url_all=url+key_code
req=urllib.request.Request(url_all)
data=urllib.request.urlopen(req).read()
print(data)æˆ–è€…å¯ä»¥å†™å…¥æ–‡ä»¶ä¹Ÿå¯ä»¥</li>
  <li>POSTè¯·æ±‚å®ä¾‹
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import urllib.request
import urllib.parse
url='http://www.iqianyue.com/mypost/'
postdata=urllib.parse.urlencode({
  'name':'yangjie',
  'pass':'12345'
}).encode('utf-8')
req=urllib.request.Request(url,postdata)
req.add_header('user-agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36')
data=urllib.request.urlopen(req).read()
fhand=open('D:/Python_code/pachong/handle/5.html','wb')
fhand.write(data)
fhand.close()
</code></pre></div>    </div>
  </li>
  <li>
    <ul>
      <li>-</li>
    </ul>
  </li>
</ul>
<font face="å®‹ä½“"> <span id="3" color="#456526"><center>ç¼–ç é—®é¢˜</center></span></font>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ç¼–ç é—®é¢˜ã€‚
Python é»˜è®¤è„šæœ¬æ–‡ä»¶éƒ½æ˜¯ ANSCII ç¼–ç çš„ï¼Œå½“æ–‡ä»¶ ä¸­æœ‰é ANSCII ç¼–ç èŒƒå›´å†…çš„å­—ç¬¦çš„æ—¶å€™å°±è¦ä½¿ç”¨â€ç¼–ç æŒ‡ç¤ºâ€æ¥ä¿®æ­£ä¸€ä¸ª module çš„å®šä¹‰ä¸­ï¼Œå¦‚æœ.pyæ–‡ä»¶ä¸­åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼ˆä¸¥æ ¼çš„è¯´æ˜¯å«æœ‰éansciiå­—ç¬¦ï¼‰ï¼Œåˆ™éœ€è¦åœ¨ç¬¬ä¸€è¡Œæˆ–ç¬¬äºŒè¡ŒæŒ‡å®šç¼–ç å£°æ˜ï¼š# -<em>- coding=utf-8 -</em>- æˆ–è€… #coding=utf-8
python byteså’Œsträ¸¤ç§ç±»å‹è½¬æ¢çš„å‡½æ•°encode(),decode()
stré€šè¿‡encode()æ–¹æ³•å¯ä»¥ç¼–ç ä¸ºæŒ‡å®šçš„bytes
åè¿‡æ¥ï¼Œå¦‚æœæˆ‘ä»¬ä»ç½‘ç»œæˆ–ç£ç›˜ä¸Šè¯»å–äº†å­—èŠ‚æµï¼Œé‚£ä¹ˆè¯»åˆ°çš„æ•°æ®å°±æ˜¯bytesã€‚è¦æŠŠbyteså˜ä¸ºstrï¼Œå°±éœ€è¦ç”¨decode()æ–¹æ³•ï¼š
æ˜¯æ•°æ®ç”¨decodeä¸æ˜¯è¿æ¥urlçš„responseï¼Œ
import urllib.request
file = urllib.request.urlopen(â€˜http://www.baidu.comâ€™)
data=file.read()
print(file.getcode())
ç»“æœä¸º 200
æˆ‘å±…ç„¶ä¸€å¼€å§‹æ‰“ç®—ç”¨data.getcode()è¿™ä¸ªgetcode()ä¸ºçŠ¶æ€ç ,åªæ˜¯ç”¨æ¥æ£€æŸ¥ä½ æ˜¯å¦è¿æ¥æˆåŠŸæ²¡æœ‰ï¼Œè€Œdataæ˜¯æ•°æ® ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚
ä¸‹é¢çœ‹ç»å…¸çš„æŸ¥çœ‹
import requests
r=requests.get(â€˜https://gs.amazon.cn/ref=as_cn_gs_topnav_reg?ld=AZCNAGSTopnavâ€™)
print(r.status_code)
å›å¤
503
å†ç”¨
print(r.encoding)
å›å¤
â€˜ISO-8859-1â€™
è¿™æ—¶æˆ‘ä»¬å†ç”¨r.encoding=r.apparent_encoding
print(r.text)
å°±èƒ½çœ‹åˆ°è¿”å›æç¤ºé”™è¯¯ï¼Œ
ï¼šæ„å¤–é”™è¯¯ç­‰
è¿™å°±æ˜¯ç½‘ç«™ä¸è®©æˆ‘ä»¬çˆ¬è™«æµè§ˆï¼Œæ‰€ä»¥å°±ä¼šæœ‰å¤´éƒ¨çš„ä¿®æ”¹</p>
      <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>æ¥ä¸€ä¸ªé”™è¯¯
import urllib.request
file = urllib.request.urlopen('http://www.baidu.com')
data=file.read()
print(data)
ç»“æœä¸º è¿™ç§é”™è¯¯ï¼Œç©ºæ ¼å’Œæ¢è¡Œè¢«æ‰“å°å‡ºäº†
b'<span class="cp">&lt;!DOCTYPE html&gt;</span>\n<span class="c">&lt;!--STATUS OK--&gt;</span>\n\r\n\r\n\r\n\r\n\r\n\
\n\n\n<span class="nt">&lt;html&gt;</span>\n<span class="nt">&lt;head&gt;</span>\n    \n    <span class="nt">&lt;meta</span> <span class="na">http-equiv=</span><span class="s">"content-type"</span> <span class="na">co</span>
<span class="err">ä½†æ˜¯æˆ‘æ”¹äº†ä¸€ç‚¹ä¹‹å</span>
<span class="err">ä½ çœ‹</span>
<span class="na">import</span> <span class="na">urllib</span><span class="err">.</span><span class="na">request</span>
<span class="na">file = </span><span class="s">urllib.request.urlopen('http://www.baidu.com')</span>
<span class="na">data=</span><span class="s">file.read()</span>
<span class="na">print</span><span class="err">(</span><span class="na">data</span><span class="err">.</span><span class="na">decode</span><span class="err">())</span>
<span class="err">å°±å¥½äº†</span> <span class="err">ï¼Œæ²¡æœ‰äº†\</span><span class="na">n</span><span class="err">å’Œ\</span><span class="na">r</span><span class="err">,è€Œä¸”æ ¼å¼éƒ½æ•´é½äº†å¦‚ä¸‹</span>
</code></pre></div>      </div>
      <p>```</p>
    </blockquote>
  </blockquote>
</blockquote>
<html>
<head>
    <meta http-equiv="content-type" content="text/html;charset=utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
   .....
åæ¥ç»è¿‡ç™¾åº¦
encodeå’Œdecodeåˆ†åˆ«æŒ‡ç¼–ç å’Œè§£ç 
åœ¨pythonä¸­ï¼ŒUnicodeç±»å‹æ˜¯ä½œä¸ºç¼–ç çš„åŸºç¡€ç±»å‹ï¼Œå³ï¼š

      decode                 encode
str ---------&gt; str(Unicode) ---------&gt; str
1
2
&gt;&gt;&gt; u = 'ä¸­æ–‡'                 # æŒ‡å®šå­—ç¬¦ä¸²ç±»å‹å¯¹è±¡u 

&gt;&gt;&gt; str1 = u.encode('gb2312')  # ä»¥gb2312ç¼–ç å¯¹uè¿›è¡Œç¼–ç ï¼Œè·å¾—bytesç±»å‹å¯¹è±¡
&gt;&gt;&gt; print(str1)
b'\xd6\xd0\xce\xc4'

&gt;&gt;&gt; str2 = u.encode('gbk')     # ä»¥gbkç¼–ç å¯¹uè¿›è¡Œç¼–ç ï¼Œè·å¾—bytesç±»å‹å¯¹è±¡
&gt;&gt;&gt; print(str2)
b'\xd6\xd0\xce\xc4'
&gt;&gt;&gt; str3 = u.encode('utf-8')   # ä»¥utf-8ç¼–ç å¯¹uè¿›è¡Œç¼–ç ï¼Œè·å¾—bytesç±»å‹å¯¹è±¡
&gt;&gt;&gt; print(str3)
b'\xe4\xb8\xad\xe6\x96\x87'

&gt;&gt;&gt; u1 = str1.decode('gb2312') # ä»¥gb2312ç¼–ç å¯¹å­—ç¬¦ä¸²strè¿›è¡Œè§£ç ï¼Œè·å¾—å­—ç¬¦ä¸²ç±»å‹å¯¹è±¡
&gt;&gt;&gt; print('u1')
'ä¸­æ–‡'

&gt;&gt;&gt; u2 = str1.decode('utf-8')  # æŠ¥é”™ï¼Œå› ä¸ºstr1æ˜¯gb2312ç¼–ç çš„
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd6 in position 0: invalid continuation byte
--------------------- 
å¯ä»¥çœ‹çœ‹è¿™ä¸ª
å»ºç«‹ä¸€ä¸ªæ–‡ä»¶test.txtï¼Œæ–‡ä»¶æ ¼å¼ç”¨ANSIï¼Œå†…å®¹ä¸º:"abcä¸­æ–‡",ç”¨pythonæ¥è¯»å–
# coding=gbk
print open("Test.txt").read()
ç»“æœï¼šabcä¸­æ–‡

æŠŠæ–‡ä»¶æ ¼å¼æ”¹æˆUTF-8ï¼š
ç»“æœï¼šabcæ¶“ æƒ

æ˜¾ç„¶ï¼Œè¿™é‡Œéœ€è¦è§£ç ï¼š

# coding=gbk
import codecs
print open("Test.txt").read().decode("utf-8")
ç»“æœï¼šabcä¸­æ–‡
è¿˜æœ‰
with  open ('beautifulSoup_set.html','r',encoding='utf8') as f:
encodingä¸ºå£°æ˜æ–‡ä»¶ç¼–ç 
 r.encoding=r.apparent_encodingä¸€èˆ¬è¿™æ ·å°±å¯ä»¥ä¸ç”¨å£°æ˜äº†
```
å†æ¥ä¸€ä¸ªä¾‹å­å§
```
é€†æ°´åŸæ®‡
pythonçˆ¬è™«è§£å†³gbkä¹±ç é—®é¢˜
ä»Šå¤©å°è¯•äº†ä¸‹çˆ¬è™«ï¼Œçˆ¬å–ä¸€æœ¬å°è¯´ï¼Œå¿˜è¯­çš„å‡¡äººä¿®ä»™ä»™ç•Œç¯‡ï¼Œå½“ç„¶è¿™æ ·ä¸å¥½ï¼Œå¤§å®¶è¦æ”¯æŒæ­£ç‰ˆã€‚

ã€€ã€€çˆ¬å–è¿‡ç¨‹ä¸­æ˜¯è€å¥—è·¯ï¼Œå…ˆè·å–ç½‘é¡µæºä»£ç ã€€ã€€

å¤åˆ¶ä»£ç 
# -*- coding:UTF-8 -*-
from bs4 import BeautifulSoup
import requests

if __name__ =='__main__':
    url='http://www.biquge.com.tw/18_18998/8750558.html'
    page_req=requests.get(url)
    html=page_req.text
    bf=BeautifulSoup( html)
    texts = bf.find_all('div',id='content')
    print(texts[0].text.replace('\xa0'*8,'\n\n'))
å¤åˆ¶ä»£ç 
ã€€ã€€ç»“æœï¼šä¹±ç 



ã€€ã€€åœ¨æµè§ˆå™¨çœ‹ä¸‹ä»£ç ï¼Œæ˜¯gbkç¼–ç ï¼Œéœ€è¦è¿›è¡Œè½¬ç ï¼Œè¿™æ–¹é¢ä¸æ¸…æ¥šï¼ŒæŸ¥äº†ä¸‹èµ„æ–™ã€‚
ã€€ã€€PSï¼šçˆ¬å–çš„æ‰€æœ‰ç½‘é¡µæ— è®ºä½•ç§ç¼–ç æ ¼å¼ï¼Œéƒ½è½¬åŒ–ä¸ºutf-8æ ¼å¼è¿›è¡Œå­˜å‚¨ï¼Œä¸æºä»£ç ç¼–ç æ ¼å¼ä¸åŒæ‰€ä»¥å‡ºç°ä¹±ç 

 

ã€€ã€€UTF-8é€šç”¨æ€§æ¯”è¾ƒå¥½ï¼Œæ˜¯ç”¨ä»¥è§£å†³å›½é™…ä¸Šå­—ç¬¦çš„ä¸€ç§å¤šå­—èŠ‚ç¼–ç ï¼Œå®ƒå¯¹è‹±æ–‡ä½¿ç”¨8ä½ï¼ˆå³ä¸€ä¸ªå­—èŠ‚ï¼‰ï¼Œä¸­æ–‡ä½¿ç”¨24ä½ï¼ˆä¸‰ä¸ªå­—èŠ‚ï¼‰æ¥ç¼–ç ã€‚

ã€€ã€€UTF-8ç¼–ç çš„æ–‡å­—å¯ä»¥åœ¨å„å›½å„ç§æ”¯æŒUTF8å­—ç¬¦é›†çš„æµè§ˆå™¨ä¸Šæ˜¾ç¤ºï¼Œä¹Ÿå°±æ˜¯å¿…é¡»ä¸¤è€…éƒ½æ˜¯utf-8æ‰è¡Œã€‚


ã€€ã€€gbkæ˜¯æ˜¯å›½å®¶ç¼–ç ï¼Œé€šç”¨æ€§æ¯”UTF8å·®ï¼ŒGB2312ä¹‹ç±»çš„éƒ½ç®—æ˜¯gbkç¼–ç ã€‚

ã€€ã€€GBKåŒ…å«å…¨éƒ¨ä¸­æ–‡å­—ç¬¦ï¼›UTF-8åˆ™åŒ…å«å…¨ä¸–ç•Œæ‰€æœ‰å›½å®¶éœ€è¦ç”¨åˆ°çš„å­—ç¬¦ã€‚


ã€€ã€€unicodeæ˜¯ä¸€ç§äºŒè¿›åˆ¶ç¼–ç ï¼Œæ‰€æœ‰utf-8å’Œgbkç¼–ç éƒ½å¾—é€šè¿‡unicodeç¼–ç è¿›è¡Œè½¬è¯‘ï¼Œå³utf-8å’Œgbkç¼–ç ä¹‹é—´ä¸èƒ½ç›´æ¥è½¬æ¢ã€‚é™„å›¾å¦‚ä¸‹ï¼š



 

ã€€ã€€pythonä¸­ç¼–ç è½¬æ¢ç”¨åˆ°äº†ä¸¤ä¸ªå‡½æ•°decodeï¼ˆ)å’Œencodeï¼ˆï¼‰
ã€€ã€€æ¯”å¦‚ï¼šhtml=page_req.text.encode('iso-8859-1').decode('utf-8')
ã€€ã€€encode('iso-8859-1') æ˜¯å°†gbkç¼–ç ç¼–ç æˆunicodeç¼–ç 
ã€€ã€€decode(â€˜gbkâ€™) æ˜¯ä»unicodeç¼–ç è§£ç æˆgbkå­—ç¬¦ä¸²

ã€€ã€€ç”±äºpycharmåªèƒ½æ˜¾ç¤ºæ¥è‡ªunicodeçš„æ±‰å­—ï¼Œä»£ç ä¿®æ”¹å¦‚ä¸‹ï¼š

å¤åˆ¶ä»£ç 
# -*- coding:UTF-8 -*-
from bs4 import BeautifulSoup
import requests

if __name__ =='__main__':
    url='http://www.biquge.com.tw/18_18998/8750558.html'
    page_req=requests.get(url)
    html=page_req.text.encode('iso-8859-1')
    bf=BeautifulSoup( html)
    texts = bf.find_all('div',id='content')
    print(texts[0].text.replace('\xa0'*8,'\n\n'))
å¤åˆ¶ä»£ç 
æœ€åè§£å†³ï¼š

æœ€åå†è¡¥å……ä¸€ç‚¹
raise_for_status()æ–¹æ³•
ç†è§£Responseç±»éå¸¸é‡è¦ï¼ŒResponseè¿™æ ·çš„ä¸€ä¸ªå¯¹è±¡è¿”å›äº†æ‰€æœ‰çš„ç½‘é¡µå†…å®¹ï¼Œé‚£ä¹ˆå®ƒä¹Ÿæä¾›äº†ä¸€ä¸ªæ–¹æ³•ï¼Œå«raise_for_status()ï¼Œè¿™ä¸ªæ–¹æ³•æ˜¯ä¸“é—¨ä¸å¼‚å¸¸æ‰“äº¤é“çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰è¿™æ ·ä¸€ä¸ªæœ‰è¶£çš„åŠŸèƒ½ï¼Œå®ƒèƒ½å¤Ÿåˆ¤æ–­è¿”å›çš„Responseç±»å‹çŠ¶æ€æ˜¯ä¸æ˜¯200ã€‚å¦‚æœæ˜¯200ï¼Œä»–å°†è¡¨ç¤ºè¿”å›çš„å†…å®¹æ˜¯æ­£ç¡®çš„ï¼Œå¦‚æœä¸æ˜¯200ï¼Œä»–å°±ä¼šäº§ç”Ÿä¸€ä¸ªHttpErrorçš„å¼‚å¸¸ã€‚
é‚£è¿™ä¸ªæ–¹æ³•æœ‰ä»€ä¹ˆç”¨å‘¢ï¼Ÿ
é‚£æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æˆ‘ä»¬çš„é€šç”¨ä»£ç æ¡†æ¶ï¼š
&gt;&gt;&gt; def getHTMLText(url):
    try:
        r = requests.get(url, timeout = 30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "äº§ç”Ÿå¼‚å¸¸"

è¿™ä¸ªä»£ç ä¸­æˆ‘ä»¬ç”¨r.raise_for_status()æ–¹æ³•ï¼Œå®ƒå°±å¯ä»¥æœ‰æ•ˆçš„åˆ¤æ–­ç½‘ç»œè¿æ¥çš„çŠ¶æ€ã€‚å¦‚æœç½‘è¿æ¥å‡ºç°é”™è¯¯ï¼Œé‚£ä¹ˆå®ƒå°±ä¼šç”¨try-exceptæ¥è·å–ä¸€ä¸ªå¼‚å¸¸ã€‚è€Œè¿™ä¸ªå¼‚å¸¸ï¼Œåœ¨å¼‚å¸¸éƒ¨åˆ†ï¼Œæˆ‘ä»¬ç”¨äº†ä¸€å¥ return â€œäº§ç”Ÿå¼‚å¸¸â€  æ¥è¡¨ç¤ºï¼Œæˆ‘ä»¬æ•è·åˆ°äº†è¿™ä¸ªå¼‚å¸¸ï¼Œæ‰€ä»¥è¿™æ ·ä¸€ä¸ªé€šç”¨ä»£ç æ¡†æ¶å¯ä»¥æœ‰æ•ˆçš„å¤„ç†ï¼Œæˆ‘ä»¬åœ¨è®¿é—®æˆ–çˆ¬å–ç½‘é¡µè¿‡ç¨‹ä¸­ï¼Œå®ƒå¯èƒ½å‡ºç°çš„ä¸€äº›é”™è¯¯ï¼Œæˆ–è€…æ˜¯ç½‘ç»œä¸ç¨³å®šé€ æˆçš„ä¸€äº›ç°è±¡ã€‚
å…¶ä»–
æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨chardet.detect()æ–¹æ³•ï¼Œåˆ¤æ–­ç½‘é¡µçš„ç¼–ç æ–¹å¼äº†ã€‚è‡³æ­¤ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç¼–å†™ä¸€ä¸ªå°ç¨‹åºåˆ¤æ–­ç½‘é¡µçš„ç¼–ç æ–¹å¼äº†ï¼Œæ–°å»ºæ–‡ä»¶åä¸ºchardet_test01.pyï¼š

# -*- coding: UTF-8 -*-
from urllib import request
import chardet

if __name__ == "__main__":
    response = request.urlopen("http://fanyi.baidu.com/")
    html = response.read()
    charset = chardet.detect(html)
    print(charset)
    ç»“æœæ˜¯ utf-8
ç„¶åå†è§£ç å°±å¯ä»¥äº†
from urllib import request

if __name__ == "__main__":
    response = request.urlopen("http://www.fanyi.baidu.com/")
    html = response.read()
    html = html.decode("utf-8")
    print(html)

```
- - -
<font face="å®‹ä½“"> <span id="4" color="#456526"><center>requests</center></span></font>
- - -
æ¥çœ‹ç¬¬ä¸€ç§requestsåº“ä»¥åŠå®ƒæ€æ ·çˆ¬å–ä¸€äº›åŸºæœ¬çš„ç½‘é¡µ
&gt;&gt;&gt; requests.request()æ„é€ ä¸€ä¸ªè¯·æ±‚
&gt;&gt;&gt;requests.get()è·å–ç½‘é¡µä¸»è¦å†…å®¹
requests.head()è·å–ç½‘é¡µå¤´éƒ¨ä¿¡æ¯
è¿˜æœ‰post(),put(),delete()ç­‰
requestsçˆ¬å–çš„é€šç”¨ä»£ç æ¡†æ¶
import requests
def getHTMLText(url):
try:
  r=reqyests.get(url,timeout=30)
  r.raise_for_status()
  r.encoding=r.apparent_encoding
  return r.text
except:
  return 'äº§ç”Ÿå¼‚å¸¸'
  (  r.raise_for_status()
  r.encoding=r.apparent_encoding
  åé¢æœ‰è§£é‡Š)
  è¯¦ç»†è§£é‡Šä¸‹
  requests.request(method,url,**kwargs)
  **kwargs:æ§åˆ¶è®¿é—®çš„å‚æ•°ï¼Œä¸ºå¯é€‰é¡¹
  paramsï¼šå­—å…¸æˆ–å­—èŠ‚ç³»åˆ—ï¼Œä½œä¸ºå‚æ•°å¢åŠ åˆ°urlä¸­
  &gt;&gt;&gt;kv = {'key1':'values1','key2':'values2'}
  &gt;&gt;&gt;r=requests.request('GET','http://python123.io/ws',params=kv)
  print(r.url)
  ç»“æœä¸º
https://python123.io/ws?key1=values1&amp;key2=values2
å¦‚æœè¦ä¸­æ–‡éœ€è¦è¿™æ ·æ“ä½œ
body='ä¸»ä½“å†…å®¹'
body1=body.encode('utf8')
è¿˜å¯ä»¥è¯»å–æ–‡ä»¶
fs={'file':open('data.xls','rb')}
r=requests.request('POST','http://python123.io/ws',files=fs)
è®¾å®šè®¿é—®ä»£ç†æœåŠ¡å™¨
pxs={
  'http':'http://uesr:pass@10.10.10.1:1234'
  'http':'http://10.10.10.1:2323'}
  r-requests.request('GET','http://www.baidu.com',proxies=pxs)


```
çœ‹å®Œäº†requests.request,æ¥çœ‹ä¸‹å…¶ä»–ç±»å‹å¦‚get
def get_html(url):
    try:
        r=requests.get(url,timeout=20)
        r.raise_for_status()
        r.encoding=r.apparent_encoding
        return r.text
    except:
        print('é”™è¯¯')
    return ''
```
ä½ ä¹Ÿè®¸ç»å¸¸æƒ³ä¸º URL çš„æŸ¥è¯¢å­—ç¬¦ä¸²(query string)ä¼ é€’æŸç§æ•°æ®ã€‚å¦‚æœä½ æ˜¯æ‰‹å·¥æ„å»º URLï¼Œé‚£ä¹ˆæ•°æ®ä¼šä»¥é”®/å€¼å¯¹çš„å½¢å¼ç½®äº URL ä¸­ï¼Œè·Ÿåœ¨ä¸€ä¸ªé—®å·çš„åé¢ã€‚ä¾‹å¦‚ï¼Œ httpbin.org/get?key=valã€‚ Requests å…è®¸ä½ ä½¿ç”¨ params å…³é”®å­—å‚æ•°ï¼Œä»¥ä¸€ä¸ªå­—ç¬¦ä¸²å­—å…¸æ¥æä¾›è¿™äº›å‚æ•°ã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œå¦‚æœä½ æƒ³ä¼ é€’ key1=value1 å’Œ key2=value2 åˆ° httpbin.org/get ï¼Œé‚£ä¹ˆä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹ä»£ç ï¼š

&gt;&gt;&gt; payload = {'key1': 'value1', 'key2': 'value2'}
&gt;&gt;&gt; r = requests.get("http://httpbin.org/get", params=payload)
é€šè¿‡æ‰“å°è¾“å‡ºè¯¥ URLï¼Œä½ èƒ½çœ‹åˆ° URL å·²è¢«æ­£ç¡®ç¼–ç ï¼š

&gt;&gt;&gt; print(r.url)
http://httpbin.org/get?key2=value2&amp;key1=value1
æ³¨æ„å­—å…¸é‡Œå€¼ä¸º None çš„é”®éƒ½ä¸ä¼šè¢«æ·»åŠ åˆ° URL çš„æŸ¥è¯¢å­—ç¬¦ä¸²é‡Œã€‚

ä½ è¿˜å¯ä»¥å°†ä¸€ä¸ªåˆ—è¡¨ä½œä¸ºå€¼ä¼ å…¥ï¼š

&gt;&gt;&gt; payload = {'key1': 'value1', 'key2': ['value2', 'value3']}
&gt;&gt;&gt; r = requests.get('http://httpbin.org/get', params=payload)
&gt;&gt;&gt; print(r.url)
http://httpbin.org/get?key1=value1&amp;key2=value2&amp;key2=value3
- 
&gt;&gt;&gt; Requests ä¼šè‡ªåŠ¨è§£ç æ¥è‡ªæœåŠ¡å™¨çš„å†…å®¹ã€‚å¤§å¤šæ•° unicode å­—ç¬¦é›†éƒ½èƒ½è¢«æ— ç¼åœ°è§£ç ã€‚
å½“ä½ è®¿é—® r.text ä¹‹æ—¶ï¼ŒRequests ä¼šä½¿ç”¨å…¶æ¨æµ‹çš„æ–‡æœ¬ç¼–ç ã€‚ä½ å¯ä»¥æ‰¾å‡º Requests ä½¿ç”¨äº†ä»€ä¹ˆç¼–ç ï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿ç”¨r.encoding å±æ€§æ¥æ”¹å˜å®ƒï¼š

&gt;&gt;&gt; r.encoding

'utf-8'
&gt;&gt;&gt; r.encoding = 'ISO-8859-1'
å¦‚æœä½ æ”¹å˜äº†ç¼–ç ï¼Œæ¯å½“ä½ è®¿é—® r.text ï¼ŒRequest éƒ½å°†ä¼šä½¿ç”¨ r.encoding çš„æ–°å€¼ã€‚ä½ å¯èƒ½å¸Œæœ›åœ¨ä½¿ç”¨ç‰¹æ®Šé€»è¾‘è®¡ç®—å‡ºæ–‡æœ¬çš„ç¼–ç çš„æƒ…å†µä¸‹æ¥ä¿®æ”¹ç¼–ç ã€‚æ¯”å¦‚ HTTP å’Œ XML è‡ªèº«å¯ä»¥æŒ‡å®šç¼–ç ã€‚è¿™æ ·çš„è¯ï¼Œä½ åº”è¯¥ä½¿ç”¨ r.content æ¥æ‰¾åˆ°ç¼–ç ï¼Œç„¶åè®¾ç½® r.encoding ä¸ºç›¸åº”çš„ç¼–ç ã€‚è¿™æ ·å°±èƒ½ä½¿ç”¨æ­£ç¡®çš„ç¼–ç è§£æ r.text äº†ã€‚

 - Requests ä¸­ä¹Ÿæœ‰ä¸€ä¸ªå†…ç½®çš„ JSON è§£ç å™¨ï¼ŒåŠ©ä½ å¤„ç† JSON æ•°æ®ï¼š

&gt;&gt;&gt; import requests

&gt;&gt;&gt; r = requests.get('https://github.com/timeline.json')
&gt;&gt;&gt; r.json()
[{u'repository': {u'open_issues': 0, u'url': 'https://github.com/...
å¦‚æœ JSON è§£ç å¤±è´¥ï¼Œ r.json() å°±ä¼šæŠ›å‡ºä¸€ä¸ªå¼‚å¸¸ã€‚ä¾‹å¦‚ï¼Œå“åº”å†…å®¹æ˜¯ 401 (Unauthorized)ï¼Œå°è¯•è®¿é—® r.json() å°†ä¼šæŠ›å‡º ValueError: No JSON object could be decoded å¼‚å¸¸ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒæˆåŠŸè°ƒç”¨ r.json() å¹¶**ä¸**æ„å‘³ç€å“åº”çš„æˆåŠŸã€‚æœ‰çš„æœåŠ¡å™¨ä¼šåœ¨å¤±è´¥çš„å“åº”ä¸­åŒ…å«ä¸€ä¸ª JSON å¯¹è±¡ï¼ˆæ¯”å¦‚ HTTP 500 çš„é”™è¯¯ç»†èŠ‚ï¼‰ã€‚è¿™ç§ JSON ä¼šè¢«è§£ç è¿”å›ã€‚è¦æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸï¼Œè¯·ä½¿ç”¨ r.raise_for_status() æˆ–è€…æ£€æŸ¥ r.status_code æ˜¯å¦å’Œä½ çš„æœŸæœ›ç›¸åŒã€‚
&gt;&gt;&gt; bad_r = requests.get('http://httpbin.org/status/404')
&gt;&gt;&gt; bad_r.status_code
404
å¦‚æœå‘é€äº†ä¸€ä¸ªé”™è¯¯è¯·æ±‚(ä¸€ä¸ª 4XX å®¢æˆ·ç«¯é”™è¯¯ï¼Œæˆ–è€… 5XX æœåŠ¡å™¨é”™è¯¯å“åº”)ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ Response.raise_for_status() æ¥æŠ›å‡ºå¼‚å¸¸ï¼š
&gt;&gt;&gt; r.raise_for_status()
None

åˆå¦‚  ä¸€ä¸ªç½‘ä¸Šçš„åˆ—å­
1import requests
2 from bs4 import BeautifulSoup
3 
4 r = requests.get("http://www.baidu.com")
5 soup = BeautifulSoup(r.text,"html.parser")
6 
7 print(soup.title)

ä½†æ˜¯ä¸çŸ¥é“å‡ºäºä»€ä¹ˆåŸå› ï¼Œè¿™æ®µä»£ç åœ¨åŒäº‹ç”µè„‘ä¸Šè¿è¡Œè‰¯å¥½ï¼Œä½†æ˜¯åœ¨æˆ‘çš„ç”µè„‘ä¸Šè¿è¡Œ å°±ä¼šå‡ºç°ä¹±ç ã€‚

ç„¶ååˆå»è¯·æ•™ï¼Œåªè¦å†è¡¥å……ä¸Šè¿™æ®µä»£ç å°±å¯ä»¥å®Œç¾è¿è¡Œäº†ã€‚

å¤åˆ¶ä»£ç 
1 import requests
2 from bs4 import BeautifulSoup
3 
4 r = requests.get("http://www.baidu.com")
5 r.raise_for_status()
6 r.encoding = r.apparent_encoding
7 soup = BeautifulSoup(r.text,"html.parser")
8 
9 print(soup.title)
è¿™æ˜¯å› ä¸ºr.apparent_ecodingä½¿å½“å‰çš„encodingå˜å¾—ä¸€è‡´
è¯¦ç»†å¯ä»¥å‚çœ‹åé¢
- - -

```
import requests
def get_content(url):
    resp=requests.get(url)
    return resp.text
url='http://jwc.sicnu.edu.cn/#'
content=get_content(url)
print('ç»“æœä¸º',content)
````
æˆ–è€…åŠ ä¸€ä¸ªåˆ—è¡¨ç´¢å¼•ï¼Œä»¥åŠå…¶ä»–æ“ä½œ
```
import requests
def get_content(url):
    respons=requests.get(url)
    return respons.text
    
url='https://www.baidu.com'
content=get_content(url)
content_len=len(content)
print('é•¿åº¦0-100',content[0:100])
print('é•¿åº¦ä¸º',content_len)
```
```
ä¹Ÿå¯ä»¥é€šè¿‡åŒä¸€ä¸ªæ–‡ä»¶å¤¹çš„å½¢å¼è°ƒç”¨å…¶ä»–æ–‡ä»¶çš„å‡½æ•°
å¦‚
import frist_get  //å¦ä¸€ä¸ªæ–‡ä»¶
url='http://www.baidu.com.cn'
thor_content=frist_get.get_content(url)
ä½†æ˜¯æœ€å¥½å¼•å…¥if__name=='__main__':
é¿å…é‡å¤æˆ–è€…å…¶ä»–é—®é¢˜
```
è¿™ä¸ªä¸Šé¢çš„éƒ½æ˜¯requestsçš„åŸºæœ¬çˆ¬å–ç½‘é¡µå†…å®¹ï¼Œä¸‹é¢æ˜¯å°†å†…å®¹å†™å…¥æ–‡ä»¶ä¸­
```
æ–¹å¼ä¸€
å†™å…¥
f1=open('home_page.html','w',encoding='utf8')
f1.write(content)
f1.close
è¯»å–
f2=open(home_page.html','r',encoding='utf8')
content_read=f2.read()
f2.close()
æ–¹å¼äºŒ
å†™å…¥
with open ('home_page.html','w',encoding='utf8')as f3:
    f3.write(content)
è¯»å–
with open('home_page.html','r',encoding='utf8')as f4:
    content_read_2=f4.read()
```
ä»¥ä¸Šå‘¢å·®åˆ«å°±æ˜¯withè¯­å¥ä¼šå¸®æˆ‘ä»¬è¿›è¡Œæ¸…ç†ï¼Œè€Œä¸”ä¸ä»…å¯ä»¥ç”¨äºæ–‡ä»¶æ“ä½œè¿˜èƒ½å…¶ä»–åœºæ™¯
ä¸‹é¢å†æ¥çœ‹ä¸€ä¸ªè¿ç”¨å­—å…¸ï¼Œåˆ—è¡¨ï¼Œå…ƒç»„ï¼Œé›†åˆï¼Œå¾ªç¯ï¼Œç­‰çš„çˆ¬å–
```
import requests
url_dict={
    'ç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾':'http://www.phei.com.cn/',
    'åœ¨çº¿èµ„æº':'http://www.phei',
    'xyz':'www.baidu.com',
    'ç½‘ä¸Šä¹¦åº—1':'http://phei.com.cn/module/goods/wssd_index.jsp',
    'ç½‘ä¸Šä¹¦åº—2':'http://phei.com.cn/module/goods/wssd_index.jsp',
}
urls_list=[
    ('ç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾','http://www.phei.com.cn/'),
     ('åœ¨çº¿èµ„æº','http://www.phei'),
    ('xyz','www.baidu.com'),
    ('ç½‘ä¸Šä¹¦åº—1','http://phei.com.cn/module/goods/wssd_index.jsp'),
    ('ç½‘ä¸Šä¹¦åº—2','http://phei.com.cn/module/goods/wssd_index.jsp'),
]
crawled_url_for_dict=set()
for ind,name in enumerate(url_dict.keys()):
    name_url=url_dict[name]
    if name_url in crawled_url_for_dict:
        print(ind,name,'å·²ç»“æŠ“å–è¿‡äº†')
    else:
        try:
            resp=requests.get(name_url)
        except Exception as e:
            print(ind,name,':',str(e)[0:50])
            continue
        content=resp.text
        crawled_url_for_dict.add(name_url)
        with open('bydict_'+name+',html','w',encoding='utf8')  as f:
            f.write(content)
            print('å®ŒæˆæŠ“å–:{} {},å†…å®¹é•¿åº¦{}'.format(ind,name,len(content)))
for u in crawled_url_for_dict:
    print(u)
print('-'*50)

crawled_url_for_list=set()
for ind,tup in enumerate(urls_list):
    name=tup[0]
    name_url=tup[1]
    if name_url in crawled_url_for_list:
        print('å·²ç»æ‰“å°è¿‡äº†')
    else:
        try:
            resp=requests.get(name_url)
        except Exception as e:
            print(ind,name,':',str(e)[0:50])
            continue
        content=resp.text
        crawled_url_for_list.add(name_url)
        with open('bylist'+name+'.html','w',encoding='utf8')as f:
            f.write(content)
            print('æŠ“å–å®Œæˆ:{}{},å†…å®¹é•¿åº¦ä¸º{}'.format(ind,name,len(content)))
for u in crawled_url_for_list:
    print(u)
```
```
ç»“æœä¸º
å®ŒæˆæŠ“å–:0 ç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾,å†…å®¹é•¿åº¦134
1 åœ¨çº¿èµ„æº : HTTPConnectionPool(host='www.phei', port=80): Max 
2 xyz : Invalid URL 'www.baidu.com': No schema supplied. P
3 ç½‘ä¸Šä¹¦åº—1 : HTTPConnectionPool(host='phei.com.cn', port=80): M
4 ç½‘ä¸Šä¹¦åº—2 : HTTPConnectionPool(host='phei.com.cn', port=80): M
http://www.phei.com.cn/
--------------------------------------------------
æŠ“å–å®Œæˆ:0ç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾,å†…å®¹é•¿åº¦ä¸º134
1 åœ¨çº¿èµ„æº : HTTPConnectionPool(host='www.phei', port=80): Max 
2 xyz : Invalid URL 'www.baidu.com': No schema supplied. P
3 ç½‘ä¸Šä¹¦åº—1 : HTTPConnectionPool(host='phei.com.cn', port=80): M
4 ç½‘ä¸Šä¹¦åº—2 : HTTPConnectionPool(host='phei.com.cn', port=80): M
http://www.phei.com.cn/
```
```
import requests
img_url='https://i0.hdslb.com/bfs/live/5934676.jpg@.webp?03132026'
response=requests.get(img_url)
print(response)
è¿”å›
&lt;Response [200]&gt;
```
å®šåˆ¶è¯·æ±‚å¤´
å¦‚æœä½ æƒ³ä¸ºè¯·æ±‚æ·»åŠ  HTTP å¤´éƒ¨ï¼Œåªè¦ç®€å•åœ°ä¼ é€’ä¸€ä¸ª dict ç»™ headers å‚æ•°å°±å¯ä»¥äº†ã€‚

ä¾‹å¦‚ï¼Œåœ¨å‰ä¸€ä¸ªç¤ºä¾‹ä¸­æˆ‘ä»¬æ²¡æœ‰æŒ‡å®š content-type:

&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'
&gt;&gt;&gt; headers = {'user-agent': 'my-app/0.0.1'}
&gt;&gt;&gt; r = requests.get(url, headers=headers)
æ³¨æ„: å®šåˆ¶ header çš„ä¼˜å…ˆçº§ä½äºæŸäº›ç‰¹å®šçš„ä¿¡æ¯æºï¼Œä¾‹å¦‚ï¼š

å¦‚æœåœ¨ .netrc ä¸­è®¾ç½®äº†ç”¨æˆ·è®¤è¯ä¿¡æ¯ï¼Œä½¿ç”¨ headers= è®¾ç½®çš„æˆæƒå°±ä¸ä¼šç”Ÿæ•ˆã€‚è€Œå¦‚æœè®¾ç½®äº†auth= å‚æ•°ï¼Œ``.netrc`` çš„è®¾ç½®å°±æ— æ•ˆäº†ã€‚
å¦‚æœè¢«é‡å®šå‘åˆ°åˆ«çš„ä¸»æœºï¼Œæˆæƒ header å°±ä¼šè¢«åˆ é™¤ã€‚
ä»£ç†æˆæƒ header ä¼šè¢« URL ä¸­æä¾›çš„ä»£ç†èº«ä»½è¦†ç›–æ‰ã€‚
åœ¨æˆ‘ä»¬èƒ½åˆ¤æ–­å†…å®¹é•¿åº¦çš„æƒ…å†µä¸‹ï¼Œheader çš„ Content-Length ä¼šè¢«æ”¹å†™ã€‚
æ›´è¿›ä¸€æ­¥è®²ï¼ŒRequests ä¸ä¼šåŸºäºå®šåˆ¶ header çš„å…·ä½“æƒ…å†µæ”¹å˜è‡ªå·±çš„è¡Œä¸ºã€‚åªä¸è¿‡åœ¨æœ€åçš„è¯·æ±‚ä¸­ï¼Œæ‰€æœ‰çš„ header ä¿¡æ¯éƒ½ä¼šè¢«ä¼ é€’è¿›å»ã€‚

æ³¨æ„: æ‰€æœ‰çš„ header å€¼å¿…é¡»æ˜¯ stringã€bytestring æˆ–è€… unicodeã€‚å°½ç®¡ä¼ é€’ unicode header ä¹Ÿæ˜¯å…è®¸çš„ï¼Œä½†ä¸å»ºè®®è¿™æ ·åšã€‚

- æ— æ³•è®¿é—®çš„ç½‘é¡µå¯ä»¥é€šè¿‡è®¾ç½®æŠ¥å¤´
import requests
r=requests.get('https://gs.amazon.cn/ref=as_cn_gs_topnav_reg?ld=AZCNAGSTopnav')
print(r.status_code)
å›å¤
503
å†ç”¨
print(r.encoding)
å›å¤
'ISO-8859-1'
è¿™æ—¶æˆ‘ä»¬å†ç”¨r.encoding=r.apparent_encoding//è¿™ä½¿å¾—å½“å‰è¿”å›çš„ä»£ç å˜å¾—æ˜¯æˆ‘ä»¬èƒ½çœ‹æ‡‚çš„ä»£ç è¿”å›ï¼Œå°±ç®—é”™è¯¯ä¹Ÿä¼šè¿”å›è€Œä¸”èƒ½çœ‹æ‡‚ï¼Œå¦‚æœæ˜¯æ­£ç¡®çš„å°±ä¼šè¿”å›è§„èŒƒæ­£ç¡®çš„ä»£ç 
print(r.text)
å°±èƒ½çœ‹åˆ°è¿”å›æç¤ºé”™è¯¯ï¼Œ
ï¼šæ„å¤–é”™è¯¯ç­‰
è¿™å°±æ˜¯ç½‘ç«™ä¸è®©æˆ‘ä»¬çˆ¬è™«æµè§ˆï¼Œæ‰€ä»¥å°±ä¼šæœ‰å¤´éƒ¨çš„ä¿®æ”¹
ä¿®æ”¹æ·»åŠ å¦‚ä¸‹
&gt;&gt;&gt;kv={'uesr-agent':'Mozilla/5.0'}
url='https://gs.amazon.cn/ref=as_cn_gs_topnav_reg?ld=AZCNAGSTopnav'
r=requests.get(url,headers=kv)
r.status_code
è¿”å›200è¯´æ˜å¯ä»¥äº†
```
å…¨éƒ¨ä»£ç å¦‚ä¸‹
import requests
url= 'http://www.amazon.cn/gp/product/B01M8L5Z3Y'
try:
    kv={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'}
    r=requests.get(url,headers=kv)
    r.raise_for_status()
    r.encoding=r.apparent_encoding
    print(r.text[0:2000])
except:
    print('çˆ¬å–å¤±è´¥')
    ç»“æœä¸º
    
    &lt;!doctype html&gt;<html class="a-no-js" data-19ax5a9jf="dingo">
    <head>
<script type="text/javascript">var ue_t0=ue_t0||+new Date();</script>
ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ç­‰
è¿æ ¼å¼ä¹Ÿæ˜¯æ­£ç¡®çš„
```
httpåè®®è¯·æ±‚
æˆ‘ä»¬åœ¨æµè§ˆå™¨ç”¨ç™¾åº¦ä¸ºä¾‹å­è¾“å…¥hello
å‘ç°ä¸ºhttps://www.baidu.com/s?wd=hello&amp;rsv_spt=1&amp;rsv_iqid=0xcbd3ca2000079cac&amp;issp=1&amp;f=8&amp;rsv_bp=1&amp;rsv_idx=2&amp;ie=utf-8&amp;tn=baiduhome_pg&amp;rsv_enter=1&amp;rsv_sug3=7&amp;rsv_sug1=6&amp;rsv_sug7=100
```
keyworld='hello'
 url='http://www.baidu.com/s?wd='+keyworld
 req=requests.get(url)
 print(req.text)
 æˆ–è€…è¿™æ ·
 import requests
kv={'wd':'python'}
r=requests.get('http://www.baidu.com/s',params=kv)
print(r.status_code)
print(r.request.url)
ï¼ˆrequestsæ¨¡å—å‘é€è¯·æ±‚æœ‰dataã€paramsä¸¤ç§æºå¸¦å‚æ•°çš„æ–¹æ³•ã€‚
paramsåœ¨getè¯·æ±‚ä¸­ä½¿ç”¨ï¼Œdataåœ¨postè¯·æ±‚ä¸­ä½¿ç”¨ã€‚ï¼‰
ç»“æœä¸º
D:\Python_code\env_pacong\Scripts\python.exe D:/Python_code/pachong/pachong1.py
200
http://www.baidu.com/s?wd=python
```
ä¸‹é¢å°†å®ƒå†™å…¥æ–‡ä»¶ ç”¨requests
```
import requests
kv={'wd':'python'}
r=requests.get('http://www.baidu.com/s',params=kv)
r.encoding=r.apparent_encoding
with open('D:/Python_code/pachong/handle/2.html','wb')as f:
    f.write(r.text)
print('ä¸‹è½½å®Œæˆ')
æŠ¥é”™TypeError: a bytes-like object is required, not 'str'
 python byteså’Œsträ¸¤ç§ç±»å‹è½¬æ¢çš„å‡½æ•°encode(),decode()

stré€šè¿‡encode()æ–¹æ³•å¯ä»¥ç¼–ç ä¸ºæŒ‡å®šçš„bytes
åè¿‡æ¥ï¼Œå¦‚æœæˆ‘ä»¬ä»ç½‘ç»œæˆ–ç£ç›˜ä¸Šè¯»å–äº†å­—èŠ‚æµï¼Œé‚£ä¹ˆè¯»åˆ°çš„æ•°æ®å°±æ˜¯bytesã€‚è¦æŠŠbyteså˜ä¸ºstrï¼Œå°±éœ€è¦ç”¨decode()æ–¹æ³•ï¼š
ç„¶è€Œåªéœ€è¦è¿™æ ·å°±è¡Œäº†
import requests
kv={'wd':'python'}
r=requests.get('http://www.baidu.com/s',params=kv)
with open('D:/Python_code/pachong/handle/2.html','wb')as f:
    f.write(r.content)
print('ä¸‹è½½å®Œæˆ')
ç„¶åæ‰“å¼€2.html æ˜¯python çš„ç™¾åº¦é¡µé¢  å¯¹çš„
   æ•´ä¸ªåªéœ€è¦æŠŠtextæ¢content
å¯èƒ½å› ä¸ºcontentæ‹¿åˆ°çš„æ˜¯äºŒè¿›åˆ¶çš„å½¢å¼è€Œtextæ˜¯å­—ç¬¦ä¸²

```
ç”¨requestsä¸‹è½½å›¾ç‰‡
```
import requests
import os
url='http://image.nationalgeographic.com.cn/2017/0211/20170211061910157.jpg'
root='D://pathon_code//pachong//'
æ–‡ä»¶æ ¹ç›®å½•
path=root+url.split('/')[-1]
ç”¨æœ€åå‘½å
try:
    if not os.path.exists(root):
        os.madir(root)
        åˆ¤æ–­ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(path):
    åˆ¤æ–­è·¯å¾„æ˜¯å¦å­˜åœ¨
        r=requests.get(url)
        with open(path,'wb') as f:
            f.write(r.content)
            f.close()
            print('æ–‡ä»¶ä¿æŒæˆåŠŸ')
    else:
        print('æ–‡ä»¶å·²ç»å­˜åœ¨')
except:
    print('çˆ¬å–å¤±è´¥')
```
&gt;&gt;&gt;ä¸‹é¢å°†è¿›è¡Œå‡ ä¸ªçˆ¬å–ä¾‹å­
1.çˆ¬å–äº¬ä¸œå•†å“ä¿¡æ¯
ä»¥ä¸‹æ˜¯æˆ‘çš„ä»£ç 
import requests
r=requests.get('https://item.jd.com/37230573868.html')
print(r.status_code)
print(r.encoding)
print(r.text[0:1000])
ä¸‹é¢æ¥çœ‹è§„èŒƒçš„ä»£ç 
import requests
url='https://item.jd.com/37230573868.html'
try:
    r=requests.get(url)
    r.raise_for_status()
    r.encoding=r.apparent_encoding
    print(r.text[:1000])
except:
    print('çˆ¬å–å¤±è´¥')

è™½ç„¶ç»“æœéƒ½ä¸€æ ·ï¼Œä½†æ˜¯è¿™æ ·é€šè¿‡ r.raise_for_status()æ’é™¤äº†å¼‚å¸¸ï¼Œå¦‚æœè¿”å›çš„ä»£ç æ˜¯200æ˜¯ä¸ä¼šæœ‰ç”¨çš„ï¼Œå¦åˆ™å°±ä¼šäº§ç”Ÿå¼‚å¸¸ï¼Œå†é€šè¿‡try exceptè¿™æ ·ç¨‹åºå¥å£®æ€§å¢å¼ºè®¸å¤šï¼Œè€Œä¸”é€šè¿‡r.encoding=r.apparent_encodingä½¿å¾—è¿”å›çš„ä»£ç æˆ‘ä»¬èƒ½çœ‹å¾—æ‡‚ï¼Œå°±ç®—é”™è¯¯ä¹Ÿä¼šå¾—åˆ°æ„å¤–é”™è¯¯çš„ä»£ç è¿”å›

&gt;&gt;&gt;ä¸‹é¢æ¥çœ‹çˆ¬å–ä¾‹å­äºŒ
import requests
url= 'http://www.amazon.cn/gp/product/B01M8L5Z3Y'
try:
    kv={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'}
    r=requests.get(url,headers=kv)
    r.raise_for_status()
    r.encoding=r.apparent_encoding
    print(r.text[0:2000])
except:
    print('çˆ¬å–å¤±è´¥')
     

æ›´åŠ å¤æ‚çš„ POST è¯·æ±‚
é€šå¸¸ï¼Œä½ æƒ³è¦å‘é€ä¸€äº›ç¼–ç ä¸ºè¡¨å•å½¢å¼çš„æ•°æ®â€”â€”éå¸¸åƒä¸€ä¸ª HTML è¡¨å•ã€‚è¦å®ç°è¿™ä¸ªï¼Œåªéœ€ç®€å•åœ°ä¼ é€’ä¸€ä¸ªå­—å…¸ç»™ data å‚æ•°ã€‚ä½ çš„æ•°æ®å­—å…¸åœ¨å‘å‡ºè¯·æ±‚æ—¶ä¼šè‡ªåŠ¨ç¼–ç ä¸ºè¡¨å•å½¢å¼ï¼š

&gt;&gt;&gt; payload = {'key1': 'value1', 'key2': 'value2'}

&gt;&gt;&gt; r = requests.post("http://httpbin.org/post", data=payload)
&gt;&gt;&gt; print(r.text)
{
  ...
  "form": {
    "key2": "value2",
    "key1": "value1"
  },
  ...
}
ä½ è¿˜å¯ä»¥ä¸º data å‚æ•°ä¼ å…¥ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ã€‚åœ¨è¡¨å•ä¸­å¤šä¸ªå…ƒç´ ä½¿ç”¨åŒä¸€ key çš„æ—¶å€™ï¼Œè¿™ç§æ–¹å¼å°¤å…¶æœ‰æ•ˆï¼š

&gt;&gt;&gt; payload = (('key1', 'value1'), ('key1', 'value2'))
&gt;&gt;&gt; r = requests.post('http://httpbin.org/post', data=payload)
&gt;&gt;&gt; print(r.text)
{
  ...
  "form": {
    "key1": [
      "value1",
      "value2"
    ]
  },
  ...
}
å¾ˆå¤šæ—¶å€™ä½ æƒ³è¦å‘é€çš„æ•°æ®å¹¶éç¼–ç ä¸ºè¡¨å•å½¢å¼çš„ã€‚å¦‚æœä½ ä¼ é€’ä¸€ä¸ª string è€Œä¸æ˜¯ä¸€ä¸ª dictï¼Œé‚£ä¹ˆæ•°æ®ä¼šè¢«ç›´æ¥å‘å¸ƒå‡ºå»ã€‚

ä¾‹å¦‚ï¼ŒGithub API v3 æ¥å—ç¼–ç ä¸º JSON çš„ POST/PATCH æ•°æ®ï¼š

&gt;&gt;&gt; import json

&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'
&gt;&gt;&gt; payload = {'some': 'data'}
&gt;&gt;&gt; r = requests.post(url, data=json.dumps(payload))
æ­¤å¤„é™¤äº†å¯ä»¥è‡ªè¡Œå¯¹ dict è¿›è¡Œç¼–ç ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨ json å‚æ•°ç›´æ¥ä¼ é€’ï¼Œç„¶åå®ƒå°±ä¼šè¢«è‡ªåŠ¨ç¼–ç ã€‚è¿™æ˜¯ 2.4.2 ç‰ˆçš„æ–°åŠ åŠŸèƒ½ï¼š

&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'
&gt;&gt;&gt; payload = {'some': 'data'}
&gt;&gt;&gt; r = requests.post(url, json=payload)
POSTä¸€ä¸ªå¤šéƒ¨åˆ†ç¼–ç (Multipart-Encoded)çš„æ–‡ä»¶
Requests ä½¿å¾—ä¸Šä¼ å¤šéƒ¨åˆ†ç¼–ç æ–‡ä»¶å˜å¾—å¾ˆç®€å•ï¼š

&gt;&gt;&gt; url = 'http://httpbin.org/post'
&gt;&gt;&gt; files = {'file': open('report.xls', 'rb')}

&gt;&gt;&gt; r = requests.post(url, files=files)
&gt;&gt;&gt; r.text
{
  ...
  "files": {
    "file": "<censored...binary...data>"
  },
  ...
}
ä½ å¯ä»¥æ˜¾å¼åœ°è®¾ç½®æ–‡ä»¶åï¼Œæ–‡ä»¶ç±»å‹å’Œè¯·æ±‚å¤´ï¼š

&gt;&gt;&gt; url = 'http://httpbin.org/post'
&gt;&gt;&gt; files = {'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', {'Expires': '0'})}

&gt;&gt;&gt; r = requests.post(url, files=files)
&gt;&gt;&gt; r.text
{
  ...
  "files": {
    "file": "<censored...binary...data>"
  },
  ...
}
å¦‚æœä½ æƒ³ï¼Œä½ ä¹Ÿå¯ä»¥å‘é€ä½œä¸ºæ–‡ä»¶æ¥æ¥æ”¶çš„å­—ç¬¦ä¸²ï¼š

&gt;&gt;&gt; url = 'http://httpbin.org/post'
&gt;&gt;&gt; files = {'file': ('report.csv', 'some,data,to,send\nanother,row,to,send\n')}

&gt;&gt;&gt; r = requests.post(url, files=files)
&gt;&gt;&gt; r.text
{
  ...
  "files": {
    "file": "some,data,to,send\\nanother,row,to,send\\n"
  },
  ...
}
å¦‚æœä½ å‘é€ä¸€ä¸ªéå¸¸å¤§çš„æ–‡ä»¶ä½œä¸º multipart/form-data è¯·æ±‚ï¼Œä½ å¯èƒ½å¸Œæœ›å°†è¯·æ±‚åšæˆæ•°æ®æµã€‚é»˜è®¤ä¸‹ requests ä¸æ”¯æŒ, ä½†æœ‰ä¸ªç¬¬ä¸‰æ–¹åŒ… requests-toolbelt æ˜¯æ”¯æŒçš„ã€‚ä½ å¯ä»¥é˜…è¯» toolbelt æ–‡æ¡£æ¥äº†è§£ä½¿ç”¨æ–¹æ³•ã€‚

åœ¨ä¸€ä¸ªè¯·æ±‚ä¸­å‘é€å¤šæ–‡ä»¶å‚è€ƒ é«˜çº§ç”¨æ³• ä¸€èŠ‚ã€‚

è­¦å‘Š
å¼ºçƒˆå»ºè®®ä½ ç”¨äºŒè¿›åˆ¶æ¨¡å¼(binary mode)æ‰“å¼€æ–‡ä»¶ã€‚è¿™æ˜¯å› ä¸º Requests å¯èƒ½ä¼šè¯•å›¾ä¸ºä½ æä¾› Content-Length headerï¼Œåœ¨å®ƒè¿™æ ·åšçš„æ—¶å€™ï¼Œè¿™ä¸ªå€¼ä¼šè¢«è®¾ä¸ºæ–‡ä»¶çš„å­—èŠ‚æ•°ï¼ˆbytesï¼‰ã€‚å¦‚æœç”¨æ–‡æœ¬æ¨¡å¼(text mode)æ‰“å¼€æ–‡ä»¶ï¼Œå°±å¯èƒ½ä¼šå‘ç”Ÿé”™è¯¯ã€‚

å“åº”çŠ¶æ€ç 
æˆ‘ä»¬å¯ä»¥æ£€æµ‹å“åº”çŠ¶æ€ç ï¼š

&gt;&gt;&gt; r = requests.get('http://httpbin.org/get')
&gt;&gt;&gt; r.status_code
200
ä¸ºæ–¹ä¾¿å¼•ç”¨ï¼ŒRequestsè¿˜é™„å¸¦äº†ä¸€ä¸ªå†…ç½®çš„çŠ¶æ€ç æŸ¥è¯¢å¯¹è±¡ï¼š

&gt;&gt;&gt; r.status_code == requests.codes.ok
True
å¦‚æœå‘é€äº†ä¸€ä¸ªé”™è¯¯è¯·æ±‚(ä¸€ä¸ª 4XX å®¢æˆ·ç«¯é”™è¯¯ï¼Œæˆ–è€… 5XX æœåŠ¡å™¨é”™è¯¯å“åº”)ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡Response.raise_for_status() æ¥æŠ›å‡ºå¼‚å¸¸ï¼š

&gt;&gt;&gt; bad_r = requests.get('http://httpbin.org/status/404')
&gt;&gt;&gt; bad_r.status_code

404

&gt;&gt;&gt; bad_r.raise_for_status()

Traceback (most recent call last):
  File "requests/models.py", line 832, in raise_for_status
    raise http_error
requests.exceptions.HTTPError: 404 Client Error
ä½†æ˜¯ï¼Œç”±äºæˆ‘ä»¬çš„ä¾‹å­ä¸­ r çš„ status_code æ˜¯ 200 ï¼Œå½“æˆ‘ä»¬è°ƒç”¨ raise_for_status() æ—¶ï¼Œå¾—åˆ°çš„æ˜¯ï¼š

&gt;&gt;&gt; r.raise_for_status()
None
ä¸€åˆ‡éƒ½æŒºå’Œè°å“ˆã€‚

å“åº”å¤´
æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ä»¥ä¸€ä¸ª Python å­—å…¸å½¢å¼å±•ç¤ºçš„æœåŠ¡å™¨å“åº”å¤´ï¼š

&gt;&gt;&gt; r.headers
{
    'content-encoding': 'gzip',
    'transfer-encoding': 'chunked',
    'connection': 'close',
    'server': 'nginx/1.0.4',
    'x-runtime': '148ms',
    'etag': '"e1ca502697e5c9317743dc078f67693f"',
    'content-type': 'application/json'
}
ä½†æ˜¯è¿™ä¸ªå­—å…¸æ¯”è¾ƒç‰¹æ®Šï¼šå®ƒæ˜¯ä»…ä¸º HTTP å¤´éƒ¨è€Œç”Ÿçš„ã€‚æ ¹æ® RFC 2616ï¼Œ HTTP å¤´éƒ¨æ˜¯å¤§å°å†™ä¸æ•æ„Ÿçš„ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»æ„å¤§å†™å½¢å¼æ¥è®¿é—®è¿™äº›å“åº”å¤´å­—æ®µï¼š

&gt;&gt;&gt; r.headers['Content-Type']
'application/json'

&gt;&gt;&gt; r.headers.get('content-type')
'application/json'
å®ƒè¿˜æœ‰ä¸€ä¸ªç‰¹æ®Šç‚¹ï¼Œé‚£å°±æ˜¯æœåŠ¡å™¨å¯ä»¥å¤šæ¬¡æ¥å—åŒä¸€ headerï¼Œæ¯æ¬¡éƒ½ä½¿ç”¨ä¸åŒçš„å€¼ã€‚ä½† Requests ä¼šå°†å®ƒä»¬åˆå¹¶ï¼Œè¿™æ ·å®ƒä»¬å°±å¯ä»¥ç”¨ä¸€ä¸ªæ˜ å°„æ¥è¡¨ç¤ºå‡ºæ¥ï¼Œå‚è§ RFC 7230:

A recipient MAY combine multiple header fields with the same field name into one "field-name: field-value" pair, without changing the semantics of the message, by appending each subsequent field value to the combined field value in order, separated by a comma.

æ¥æ”¶è€…å¯ä»¥åˆå¹¶å¤šä¸ªç›¸åŒåç§°çš„ header æ ä½ï¼ŒæŠŠå®ƒä»¬åˆä¸ºä¸€ä¸ª "field-name: field-value" é…å¯¹ï¼Œå°†æ¯ä¸ªåç»­çš„æ ä½å€¼ä¾æ¬¡è¿½åŠ åˆ°åˆå¹¶çš„æ ä½å€¼ä¸­ï¼Œç”¨é€—å·éš”å¼€å³å¯ï¼Œè¿™æ ·åšä¸ä¼šæ”¹å˜ä¿¡æ¯çš„è¯­ä¹‰ã€‚

Cookie
å¦‚æœæŸä¸ªå“åº”ä¸­åŒ…å«ä¸€äº› cookieï¼Œä½ å¯ä»¥å¿«é€Ÿè®¿é—®å®ƒä»¬ï¼š

&gt;&gt;&gt; url = 'http://example.com/some/cookie/setting/url'
&gt;&gt;&gt; r = requests.get(url)

&gt;&gt;&gt; r.cookies['example_cookie_name']
'example_cookie_value'
è¦æƒ³å‘é€ä½ çš„cookiesåˆ°æœåŠ¡å™¨ï¼Œå¯ä»¥ä½¿ç”¨ cookies å‚æ•°ï¼š

&gt;&gt;&gt; url = 'http://httpbin.org/cookies'
&gt;&gt;&gt; cookies = dict(cookies_are='working')

&gt;&gt;&gt; r = requests.get(url, cookies=cookies)
&gt;&gt;&gt; r.text
'{"cookies": {"cookies_are": "working"}}'
Cookie çš„è¿”å›å¯¹è±¡ä¸º RequestsCookieJarï¼Œå®ƒçš„è¡Œä¸ºå’Œå­—å…¸ç±»ä¼¼ï¼Œä½†ç•Œé¢æ›´ä¸ºå®Œæ•´ï¼Œé€‚åˆè·¨åŸŸåè·¨è·¯å¾„ä½¿ç”¨ã€‚ä½ è¿˜å¯ä»¥æŠŠ Cookie Jar ä¼ åˆ° Requests ä¸­ï¼š

&gt;&gt;&gt; jar = requests.cookies.RequestsCookieJar()
&gt;&gt;&gt; jar.set('tasty_cookie', 'yum', domain='httpbin.org', path='/cookies')
&gt;&gt;&gt; jar.set('gross_cookie', 'blech', domain='httpbin.org', path='/elsewhere')
&gt;&gt;&gt; url = 'http://httpbin.org/cookies'
&gt;&gt;&gt; r = requests.get(url, cookies=jar)
&gt;&gt;&gt; r.text
'{"cookies": {"tasty_cookie": "yum"}}'
é‡å®šå‘ä¸è¯·æ±‚å†å²
é»˜è®¤æƒ…å†µä¸‹ï¼Œé™¤äº† HEAD, Requests ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰é‡å®šå‘ã€‚

å¯ä»¥ä½¿ç”¨å“åº”å¯¹è±¡çš„ history æ–¹æ³•æ¥è¿½è¸ªé‡å®šå‘ã€‚

Response.history æ˜¯ä¸€ä¸ª Response å¯¹è±¡çš„åˆ—è¡¨ï¼Œä¸ºäº†å®Œæˆè¯·æ±‚è€Œåˆ›å»ºäº†è¿™äº›å¯¹è±¡ã€‚è¿™ä¸ªå¯¹è±¡åˆ—è¡¨æŒ‰ç…§ä»æœ€è€åˆ°æœ€è¿‘çš„è¯·æ±‚è¿›è¡Œæ’åºã€‚

ä¾‹å¦‚ï¼ŒGithub å°†æ‰€æœ‰çš„ HTTP è¯·æ±‚é‡å®šå‘åˆ° HTTPSï¼š

&gt;&gt;&gt; r = requests.get('http://github.com')

&gt;&gt;&gt; r.url
'https://github.com/'

&gt;&gt;&gt; r.status_code
200

&gt;&gt;&gt; r.history
[&lt;Response [301]&gt;]
å¦‚æœä½ ä½¿ç”¨çš„æ˜¯GETã€OPTIONSã€POSTã€PUTã€PATCH æˆ–è€… DELETEï¼Œé‚£ä¹ˆä½ å¯ä»¥é€šè¿‡ allow_redirects å‚æ•°ç¦ç”¨é‡å®šå‘å¤„ç†ï¼š

&gt;&gt;&gt; r = requests.get('http://github.com', allow_redirects=False)
&gt;&gt;&gt; r.status_code
301
&gt;&gt;&gt; r.history
[]
å¦‚æœä½ ä½¿ç”¨äº† HEADï¼Œä½ ä¹Ÿå¯ä»¥å¯ç”¨é‡å®šå‘ï¼š

&gt;&gt;&gt; r = requests.head('http://github.com', allow_redirects=True)
&gt;&gt;&gt; r.url
'https://github.com/'
&gt;&gt;&gt; r.history
[&lt;Response [301]&gt;]
è¶…æ—¶
ä½ å¯ä»¥å‘Šè¯‰ requests åœ¨ç»è¿‡ä»¥ timeout å‚æ•°è®¾å®šçš„ç§’æ•°æ—¶é—´ä¹‹ååœæ­¢ç­‰å¾…å“åº”ã€‚åŸºæœ¬ä¸Šæ‰€æœ‰çš„ç”Ÿäº§ä»£ç éƒ½åº”è¯¥ä½¿ç”¨è¿™ä¸€å‚æ•°ã€‚å¦‚æœä¸ä½¿ç”¨ï¼Œä½ çš„ç¨‹åºå¯èƒ½ä¼šæ°¸è¿œå¤±å»å“åº”ï¼š

&gt;&gt;&gt; requests.get('http://github.com', timeout=0.001)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
requests.exceptions.Timeout: HTTPConnectionPool(host='github.com', port=80): Request timed out. (timeout=0.001)
æ³¨æ„
timeout ä»…å¯¹è¿æ¥è¿‡ç¨‹æœ‰æ•ˆï¼Œä¸å“åº”ä½“çš„ä¸‹è½½æ— å…³ã€‚ timeout å¹¶ä¸æ˜¯æ•´ä¸ªä¸‹è½½å“åº”çš„æ—¶é—´é™åˆ¶ï¼Œè€Œæ˜¯å¦‚æœæœåŠ¡å™¨åœ¨ timeout ç§’å†…æ²¡æœ‰åº”ç­”ï¼Œå°†ä¼šå¼•å‘ä¸€ä¸ªå¼‚å¸¸ï¼ˆæ›´ç²¾ç¡®åœ°è¯´ï¼Œæ˜¯åœ¨timeout ç§’å†…æ²¡æœ‰ä»åŸºç¡€å¥—æ¥å­—ä¸Šæ¥æ”¶åˆ°ä»»ä½•å­—èŠ‚çš„æ•°æ®æ—¶ï¼‰If no timeout is specified explicitly, requests do not time out.

é”™è¯¯ä¸å¼‚å¸¸
é‡åˆ°ç½‘ç»œé—®é¢˜ï¼ˆå¦‚ï¼šDNS æŸ¥è¯¢å¤±è´¥ã€æ‹’ç»è¿æ¥ç­‰ï¼‰æ—¶ï¼ŒRequests ä¼šæŠ›å‡ºä¸€ä¸ª ConnectionError å¼‚å¸¸ã€‚

å¦‚æœ HTTP è¯·æ±‚è¿”å›äº†ä¸æˆåŠŸçš„çŠ¶æ€ç ï¼Œ Response.raise_for_status() ä¼šæŠ›å‡ºä¸€ä¸ª HTTPErrorå¼‚å¸¸ã€‚

è‹¥è¯·æ±‚è¶…æ—¶ï¼Œåˆ™æŠ›å‡ºä¸€ä¸ª Timeout å¼‚å¸¸ã€‚

è‹¥è¯·æ±‚è¶…è¿‡äº†è®¾å®šçš„æœ€å¤§é‡å®šå‘æ¬¡æ•°ï¼Œåˆ™ä¼šæŠ›å‡ºä¸€ä¸ª TooManyRedirects å¼‚å¸¸ã€‚

æ‰€æœ‰Requestsæ˜¾å¼æŠ›å‡ºçš„å¼‚å¸¸éƒ½ç»§æ‰¿è‡ª requests.exceptions.RequestException ã€‚



- - -


- - - 
<font face="å®‹ä½“"> <span id="5" color="#456526"><center>æ­£åˆ™å’ŒBeautifulSoupçš„è§£æ</center></span></font>
-
                æ­£åˆ™è¡¨è¾¾å¼
ä¸¤ä¸ªç‰¹æ®Šçš„ç¬¦å·'^'å’Œ'$'ã€‚ä»–ä»¬çš„ä½œç”¨æ˜¯åˆ†åˆ«æŒ‡å‡ºä¸€ä¸ªå­—ç¬¦ä¸²çš„å¼€å§‹å’Œç»“æŸã€‚ä¾‹å­å¦‚ä¸‹ï¼š

"^The"ï¼šè¡¨ç¤ºæ‰€æœ‰ä»¥"The"å¼€å§‹çš„å­—ç¬¦ä¸²ï¼ˆ"There"ï¼Œ"The cat"ç­‰ï¼‰ï¼›
"of despair$"ï¼šè¡¨ç¤ºæ‰€ä»¥ä»¥"of despair"ç»“å°¾çš„å­—ç¬¦ä¸²ï¼›
"^abc$"ï¼šè¡¨ç¤ºå¼€å§‹å’Œç»“å°¾éƒ½æ˜¯"abc"çš„å­—ç¬¦ä¸²â€”â€”å‘µå‘µï¼Œåªæœ‰"abc"è‡ªå·±äº†ï¼›
"notice"ï¼šè¡¨ç¤ºä»»ä½•åŒ…å«"notice"çš„å­—ç¬¦ä¸²ã€‚

è±¡æœ€åé‚£ä¸ªä¾‹å­ï¼Œå¦‚æœä½ ä¸ä½¿ç”¨ä¸¤ä¸ªç‰¹æ®Šå­—ç¬¦ï¼Œä½ å°±åœ¨è¡¨ç¤ºè¦æŸ¥æ‰¾çš„ä¸²åœ¨è¢«æŸ¥æ‰¾ä¸²çš„ä»»æ„éƒ¨åˆ†â€”â€”ä½ å¹¶
ä¸æŠŠå®ƒå®šä½åœ¨æŸä¸€ä¸ªé¡¶ç«¯ã€‚

å…¶å®ƒè¿˜æœ‰'*'ï¼Œ'+'å’Œ'?'è¿™ä¸‰ä¸ªç¬¦å·ï¼Œè¡¨ç¤ºä¸€ä¸ªæˆ–ä¸€åºåˆ—å­—ç¬¦é‡å¤å‡ºç°çš„æ¬¡æ•°ã€‚å®ƒä»¬åˆ†åˆ«è¡¨ç¤ºâ€œæ²¡æœ‰æˆ–
æ›´å¤šâ€ï¼Œâ€œä¸€æ¬¡æˆ–æ›´å¤šâ€è¿˜æœ‰â€œæ²¡æœ‰æˆ–ä¸€æ¬¡â€ã€‚ä¸‹é¢æ˜¯å‡ ä¸ªä¾‹å­ï¼š

"ab*"ï¼šè¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²æœ‰ä¸€ä¸ªaåé¢è·Ÿç€é›¶ä¸ªæˆ–è‹¥å¹²ä¸ªbã€‚ï¼ˆ"a", "ab", "abbb",â€¦â€¦ï¼‰ï¼›
"ab+"ï¼šè¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²æœ‰ä¸€ä¸ªaåé¢è·Ÿç€è‡³å°‘ä¸€ä¸ªbæˆ–è€…æ›´å¤šï¼›
"ab?"ï¼šè¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²æœ‰ä¸€ä¸ªaåé¢è·Ÿç€é›¶ä¸ªæˆ–è€…ä¸€ä¸ªbï¼›
"a?b+$"ï¼šè¡¨ç¤ºåœ¨å­—ç¬¦ä¸²çš„æœ«å°¾æœ‰é›¶ä¸ªæˆ–ä¸€ä¸ªaè·Ÿç€ä¸€ä¸ªæˆ–å‡ ä¸ªbã€‚

ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨èŒƒå›´ï¼Œç”¨å¤§æ‹¬å·æ‹¬èµ·ï¼Œç”¨ä»¥è¡¨ç¤ºé‡å¤æ¬¡æ•°çš„èŒƒå›´ã€‚

"ab{2}"ï¼šè¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²æœ‰ä¸€ä¸ªaè·Ÿç€2ä¸ªbï¼ˆ"abb"ï¼‰ï¼›
"ab{2,}"ï¼šè¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²æœ‰ä¸€ä¸ªaè·Ÿç€è‡³å°‘2ä¸ªbï¼›
"ab{3,5}"ï¼šè¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²æœ‰ä¸€ä¸ªaè·Ÿç€3åˆ°5ä¸ªbã€‚

è¯·æ³¨æ„ï¼Œä½ å¿…é¡»æŒ‡å®šèŒƒå›´çš„ä¸‹é™ï¼ˆå¦‚ï¼š"{0,2}"è€Œä¸æ˜¯"{,2}"ï¼‰ã€‚è¿˜æœ‰ï¼Œä½ å¯èƒ½æ³¨æ„åˆ°äº†ï¼Œ'*'ï¼Œ'+'å’Œ
'?'ç›¸å½“äº"{0,}"ï¼Œ"{1,}"å’Œ"{0,1}"ã€‚
è¿˜æœ‰ä¸€ä¸ª'Â¦'ï¼Œè¡¨ç¤ºâ€œæˆ–â€æ“ä½œï¼š

"hiÂ¦hello"ï¼šè¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²é‡Œæœ‰"hi"æˆ–è€…"hello"ï¼›
"(bÂ¦cd)ef"ï¼šè¡¨ç¤º"bef"æˆ–"cdef"ï¼›
"(aÂ¦b)*c"ï¼šè¡¨ç¤ºä¸€ä¸²"a""b"æ··åˆçš„å­—ç¬¦ä¸²åé¢è·Ÿä¸€ä¸ª"c"ï¼›

'.'å¯ä»¥æ›¿ä»£ä»»ä½•å­—ç¬¦ï¼š

"a.[0-9]"ï¼šè¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²æœ‰ä¸€ä¸ª"a"åé¢è·Ÿç€ä¸€ä¸ªä»»æ„å­—ç¬¦å’Œä¸€ä¸ªæ•°å­—ï¼›
"^.{3}$"ï¼šè¡¨ç¤ºæœ‰ä»»æ„ä¸‰ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸²ï¼ˆé•¿åº¦ä¸º3ä¸ªå­—ç¬¦ï¼‰ï¼›

æ–¹æ‹¬å·è¡¨ç¤ºæŸäº›å­—ç¬¦å…è®¸åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­çš„æŸä¸€ç‰¹å®šä½ç½®å‡ºç°ï¼š
```
"[ab]"ï¼šè¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²æœ‰ä¸€ä¸ª"a"æˆ–"b"ï¼ˆç›¸å½“äº"aÂ¦b"ï¼‰ï¼›
"[a-d]"ï¼šè¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²åŒ…å«å°å†™çš„'a'åˆ°'d'ä¸­çš„ä¸€ä¸ªï¼ˆç›¸å½“äº"aÂ¦bÂ¦cÂ¦d"æˆ–è€…"[abcd]"ï¼‰ï¼›
"^[a-zA-Z]"ï¼šè¡¨ç¤ºä¸€ä¸ªä»¥å­—æ¯å¼€å¤´çš„å­—ç¬¦ä¸²ï¼›
"[0-9]%"ï¼šè¡¨ç¤ºä¸€ä¸ªç™¾åˆ†å·å‰æœ‰ä¸€ä½çš„æ•°å­—ï¼›
",[a-zA-Z0-9]$"ï¼šè¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²ä»¥ä¸€ä¸ªé€—å·åé¢è·Ÿç€ä¸€ä¸ªå­—æ¯æˆ–æ•°å­—ç»“æŸã€‚
```
ä½ ä¹Ÿå¯ä»¥åœ¨æ–¹æ‹¬å·é‡Œç”¨'^'è¡¨ç¤ºä¸å¸Œæœ›å‡ºç°çš„å­—ç¬¦ï¼Œ'^'åº”åœ¨æ–¹æ‹¬å·é‡Œçš„ç¬¬ä¸€ä½ã€‚ï¼ˆå¦‚ï¼š"%[^a-zA-Z]%"è¡¨ç¤ºä¸¤ä¸ªç™¾åˆ†å·ä¸­ä¸åº”è¯¥å‡ºç°å­—æ¯ï¼‰ã€‚

ä¸ºäº†é€å­—è¡¨è¾¾ï¼Œä½ å¿…é¡»åœ¨"^.$()Â¦*+?{\"è¿™äº›å­—ç¬¦å‰åŠ ä¸Šè½¬ç§»å­—ç¬¦'\'ã€‚

è¯·æ³¨æ„åœ¨æ–¹æ‹¬å·ä¸­ï¼Œä¸éœ€è¦è½¬ä¹‰å­—ç¬¦ã€‚

2.æ­£åˆ™è¡¨è¾¾å¼éªŒè¯æ§åˆ¶æ–‡æœ¬æ¡†çš„è¾“å…¥å­—ç¬¦ç±»å‹
```
1.åªèƒ½è¾“å…¥æ•°å­—å’Œè‹±æ–‡çš„ï¼š 
<input onkeyup="value=value.replace(/[\W]/g,'') " onbeforepaste="clipboardData.setData('text',clipboardData.getData('text').replace(/[^\d]/g,''))" id="Text1" name="Text1" />

2.åªèƒ½è¾“å…¥æ•°å­—çš„ï¼š 
<input onkeyup="value=value.replace(/[^\d]/g,'') " onbeforepaste="clipboardData.setData('text',clipboardData.getData('text').replace(/[^\d]/g,''))" id="Text2" name="Text2" />

3.åªèƒ½è¾“å…¥å…¨è§’çš„ï¼š 
<input onkeyup="value=value.replace(/[^\uFF00-\uFFFF]/g,'')" onbeforepaste="clipboardData.setData('text',clipboardData.getData('text').replace(/[^\uFF00-\uFFFF]/g,''))" id="Text3" name="Text3" />

4.åªèƒ½è¾“å…¥æ±‰å­—çš„ï¼š 
<input onkeyup="value=value.replace(/[^\u4E00-\u9FA5]/g,'')" onbeforepaste="clipboardData.setData('text',clipboardData.getData('text').replace(/[^\u4E00-\u9FA5]/g,''))" id="Text4" name="Text4" />
```

3.æ­£åˆ™è¡¨è¾¾å¼çš„åº”ç”¨å®ä¾‹é€šä¿—è¯´æ˜
 
*******************************************************************************

//æ ¡éªŒæ˜¯å¦å…¨ç”±æ•°å­—ç»„æˆ
```
/^[0-9]{1,20}$/
```
 

^ è¡¨ç¤ºæ‰“å¤´çš„å­—ç¬¦è¦åŒ¹é…ç´§è·Ÿ^åé¢çš„è§„åˆ™

$ è¡¨ç¤ºæ‰“å¤´çš„å­—ç¬¦è¦åŒ¹é…ç´§é $å‰é¢çš„è§„åˆ™

[ ] ä¸­çš„å†…å®¹æ˜¯å¯é€‰å­—ç¬¦é›†

[0-9] è¡¨ç¤ºè¦æ±‚å­—ç¬¦èŒƒå›´åœ¨0-9ä¹‹é—´

{1,20}è¡¨ç¤ºæ•°å­—å­—ç¬¦ä¸²é•¿åº¦åˆæ³•ä¸º1åˆ°20ï¼Œå³ä¸º[0-9]ä¸­çš„å­—ç¬¦å‡ºç°æ¬¡æ•°çš„èŒƒå›´æ˜¯1åˆ°20æ¬¡ã€‚

 

/^ å’Œ $/æˆå¯¹ä½¿ç”¨åº”è¯¥æ˜¯è¡¨ç¤ºè¦æ±‚æ•´ä¸ªå­—ç¬¦ä¸²å®Œå…¨åŒ¹é…å®šä¹‰çš„è§„åˆ™ï¼Œè€Œä¸æ˜¯åªåŒ¹é…å­—ç¬¦ä¸²ä¸­çš„ä¸€ä¸ªå­ä¸²ã€‚

 

*******************************************************************************

//æ ¡éªŒç™»å½•åï¼šåªèƒ½è¾“å…¥5-20ä¸ªä»¥å­—æ¯å¼€å¤´ã€å¯å¸¦æ•°å­—ã€â€œ_â€ã€â€œ.â€çš„å­—ä¸²
```
/^[a-zA-Z]{1}([a-zA-Z0-9]|[._]){4,19}$/
```
 
```
^[a-zA-Z]{1} è¡¨ç¤ºç¬¬ä¸€ä¸ªå­—ç¬¦è¦æ±‚æ˜¯å­—æ¯ã€‚
```
```
([a-zA-Z0-9]|[._]){4,19} 
```
è¡¨ç¤ºä»ç¬¬äºŒä½å¼€å§‹ï¼ˆå› ä¸ºå®ƒç´§è·Ÿåœ¨ä¸Šä¸ªè¡¨è¾¾å¼åé¢ï¼‰çš„ä¸€ä¸ªé•¿åº¦ä¸º4åˆ°9ä½çš„å­—ç¬¦ä¸²ï¼Œå®ƒè¦æ±‚æ˜¯ç”±å¤§å°å†™å­—æ¯ã€æ•°å­—æˆ–è€…ç‰¹æ®Šå­—ç¬¦é›†[._]ç»„æˆã€‚

 

*******************************************************************************

//æ ¡éªŒç”¨æˆ·å§“åï¼šåªèƒ½è¾“å…¥1-30ä¸ªä»¥å­—æ¯å¼€å¤´çš„å­—ä¸²
```
/^[a-zA-Z]{1,30}$/
```
 

*******************************************************************************

//æ ¡éªŒå¯†ç ï¼šåªèƒ½è¾“å…¥6-20ä¸ªå­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿
```
/^(\w){6,20}$/
```
 

\wï¼šç”¨äºåŒ¹é…å­—æ¯ï¼Œæ•°å­—æˆ–ä¸‹åˆ’çº¿å­—ç¬¦


//æ ¡éªŒæ™®é€šç”µè¯ã€ä¼ çœŸå·ç ï¼šå¯ä»¥â€œ+â€æˆ–æ•°å­—å¼€å¤´ï¼Œå¯å«æœ‰â€œ-â€ å’Œ â€œ â€
```
/^[+]{0,1}(\d){1,3}[ ]?([-]?((\d)|[ ]){1,12})+$/
```
 

\dï¼šç”¨äºåŒ¹é…ä»0åˆ°9çš„æ•°å­—ï¼›

â€œ?â€å…ƒå­—ç¬¦è§„å®šå…¶å‰å¯¼å¯¹è±¡å¿…é¡»åœ¨ç›®æ ‡å¯¹è±¡ä¸­è¿ç»­å‡ºç°é›¶æ¬¡æˆ–ä¸€æ¬¡

 

å¯ä»¥åŒ¹é…çš„å­—ç¬¦ä¸²å¦‚ï¼š+123 -999 999 ï¼› +123-999 999 ï¼›123 999 999 ï¼›+123 999999ç­‰



//æ ¡éªŒURL
```
/^http[s]{0,1}:\/\/.+$/ æˆ– /^http[s]{0,1}:\/\/.{1,n}$/ (è¡¨ç¤ºurlä¸²çš„é•¿åº¦ä¸ºlength(â€œhttps://â€) + n )
\ / ï¼šè¡¨ç¤ºå­—ç¬¦â€œ/â€ã€‚
. è¡¨ç¤ºæ‰€æœ‰å­—ç¬¦çš„é›†
+ ç­‰åŒäº{1,}ï¼Œå°±æ˜¯1åˆ°æ­£æ— ç©·å§ã€‚

å†æ¥ä¸€ä¸ªæ£€éªŒä»¥.com .cnç»“å°¾url
import re
pattern='[a-zA-Z]+://[^\s]*[.com|.cn]'
string="<a href="http://www.baidu.com">ç™¾åº¦é¦–é¡µ</a>"
result=re.search(pattern,string)
print(result)
\sä¸ºç©ºç™½å­—ç¬¦
[^\s]åˆ™ä¸ºéç©ºç™½å­—ç¬¦
*åŒ¹é…ä¸€æ¬¡æˆ–å¤šæ¬¡å‰é¢çš„å­—ç¬¦
```

ä¸‹é¢å†æ¥çœ‹ä¸€ä¸ªä¾‹å­
```
import re
pattern1='p.*y' #è´ªå©ªæ¨¡å¼
patton2='p.*?y' #æ‡’æƒ°æ¨¡å¼
string ='abcdfphp124pythony_py'
result1=re.search(pattern1,string)
result2=re.search(patton2,string)
print(result1)
print(result2)
ç»“æœä¸º
D:\Python_code\env_pacong\Scripts\python.exe D:/Python_code/pachong/pachong1.py
&lt;re.Match object; span=(5, 21), match='php124pythony_py'&gt;
&lt;re.Match object; span=(5, 13), match='php124py'&gt;
```
```
import re
string='adadjgdfsdkdasadfksskaasadadyfhlmbnlvzxxaewrtl'
pattern='.a.'
result=re.compile(pattern).findall(string)
print(result)
ç»“æœä¸º
['dad', 'das', 'kaa', 'sad', 'xae']
```
å†æ¥çœ‹ä¸€ä¸ªäº¬ä¸œå•†å“çš„ç»å…¸ä¾‹å­
```
import urllib.request
import re
from bs4 import BeautifulSoup


def crawl(url,page):
    headers={'url-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'}
    req=urllib.request.Request(url=url,headers=headers)
    html=urllib.request.urlopen(req).read()
    html1=html.decode() #æ ¼å¼å˜å¾—æ ‡å‡†åŒ–ã€‚è§£ç 
    part1 = r'&lt;div id="J_goodsList" .+?<div class="m-aside">'
    result1 = re.compile(part1,flags=re.DOTALL).findall(html1)
    result1=result1[0] #ä½¿æ•°æ®å˜å¾—å·¥æ•´è€Œä¸”æ ¼å¼åŒ–äº† ä¹ˆä»¥å¶äº†/nç­‰é‡å¤
    part2=r'<img width="220" height="220" class="err-product" data-img="1" source-data-lazy-img="//(.+?\.jpg)" />'
    imagelist=re.compile(part2).findall(result1)
    x=1
    for imageurl in imagelist:
        imagename="D:/python_code/pachong/handle/jd_image/"+str(page)+str(x)+'.jpg'
        imageurl='http://'+imageurl
        try:
            urllib.request.urlretrieve(imageurl,filename=imagename)
        except urllib.error.URLError as e:
            if hasattr(e,'code'):
                x+=1
            if hasattr(e,'reason'):
                x+=1
        x+=1

for i in range(1,5):
    url='https://search.jd.com/Search?keyword=iphonex&amp;enc=utf-8&amp;qrst=1&amp;rt=1&amp;stop=1&amp;vt=2&amp;suggest=1.his.0.0&amp;page='+str(i)
    crawl(url,i)
    print('ç¬¬içˆ¬å–æˆåŠŸ')

ç»è¿‡è‰°éš¾çš„ç»ˆäºå†™å¥½è¿™ç”¨ç¬¬ä¸€ä¸ªçˆ¬è™«çˆ¬å–å›¾ç‰‡
ä¸‹é¢å›é¡¾ä¸‹å†™çš„æ—¶å€™çš„é—®é¢˜
1æ­£åˆ™è¡¨è¾¾å¼å‡ºç°äº†é—®é¢˜
æˆ‘åœ¨äº¬ä¸œå•†å“çš„é¡µé¢æºä»£ç ç”¨ctrl+fä¹Ÿæ‰¾äº†
ä¸‹é¢çš„æ ‡å¿—åªæœ‰ä¸€ä¸ªä¹Ÿå°±æ˜¯åªæœ‰ä¸€æ¬¡å‡ºç°æ­£å¥½æ»¡è¶³æ¡ä»¶
part1 = r'&lt;div id="J_goodsList" .+?<div class="m-aside">'
result1 = re.compile(part1).findall(html1)
ä¸€å¼€å§‹æˆ‘æ˜¯è¿™æ ·å†™çš„ï¼Œä½†æ˜¯ä¸€ç›´è¿”å›æˆ‘ä¸€ä¸ª[]ç„¶åç™¾åº¦å¾ˆä¹…ç»ˆäºçŸ¥é“ä¸ºä»€ä¹ˆ
è¿™ä¸ªé—®é¢˜ä¸€èˆ¬å‡ºç°åœ¨å¸Œæœ›ä½¿ç”¨å¥ç‚¹(.)æ¥åŒ¹é…ä»»æ„å­—ç¬¦ï¼Œä½†æ˜¯å¿˜è®°äº†å¥ç‚¹å¹¶ä¸èƒ½åŒ¹é…æ¢è¡Œç¬¦æ—¶:
import re
comment = re.compile(r'/\*(.*?)\*/')  # åŒ¹é…Cçš„æ³¨é‡Š
text1 = '/* this is a comment */'
text2 = """/*this is a 
    multiline comment */"""

comment.findall(text1)
Out[4]: [' this is a comment ']

comment.findall(text2)  # ç”±äºtext2æ–‡æœ¬æ¢è¡Œäº†ï¼Œæ²¡åŒ¹é…åˆ°
Out[5]: []

è§£å†³æ–¹æ³•1ï¼šæ·»åŠ å¯¹æ¢è¡Œç¬¦çš„æ”¯æŒï¼Œ(?:.|\n)æŒ‡å®šäº†ä¸€ä¸ªéæ•è·ç»„(å³ï¼Œè¿™ä¸ªç»„åªåšåŒ¹é…ä½†ä¸æ•è·ç»“æœï¼Œä¹Ÿä¸ä¼šåˆ†é…ç»„å·)
comment = re.compile(r'\*((?:.|\n)*?)\*/') 
comment.findall(text2)
Out[7]: ['this is a \n    multiline comment ']

è§£å†³æ–¹æ³•2ï¼šre.compile()å‡½æ•°å¯æ¥å—ä¸€ä¸ªæœ‰ç”¨çš„æ ‡è®°--re.DOTALLã€‚è¿™ä½¿å¾—æ­£åˆ™è¡¨è¾¾å¼ä¸­çš„å¥ç‚¹(.)å¯ä»¥åŒ¹é…æ‰€æœ‰çš„å­—ç¬¦ï¼Œä¹ŸåŒ…æ‹¬æ¢è¡Œç¬¦
comment = re.compile(r'/\*(.*?)\*/', flags=re.DOTALL)
comment.findall(text2)
Out[10]: ['this is a \n    multiline comment ']
å¾ˆæ˜æ˜¾æˆ‘å°±æ˜¯ç”¨çš„ç¬¬äºŒç§ç„¶åç»ˆäºæ­£åˆ™è¡¨è¾¾å¼æˆåŠŸçš„åŒ¹é…äº†æˆ‘æƒ³è¦çš„
çˆ¬å–äº†æƒ³è¦çš„å¤§æ¦‚å†…å®¹ä½†æ˜¯è¿˜ä¸æ˜¯æˆ‘æƒ³è¦çš„å›¾ç‰‡url
ä½†æ˜¯å‘ç°æ¯ä¸ªé¡µé¢çš„å›¾ç‰‡URLéƒ½åœ¨<img width="220" height="220" class="err-product" data-img="1" source-data-lazy-img="//img12.360buyimg.com/n7/jfs/t1/38769/36/65/76968/5cb83ffeEe5900ad2/ee8924e4c5a0fd03.jpg" />
è¿™é‡Œé¢ï¼Œæˆ‘ç”¨ctrl+fæœç´¢source-data-lazy-img=æ‰¾åˆ°ä»–ä»¬
ä½†æ˜¯æ€ä¹ˆç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰¾å‡ºä»–ä»¬é‚£ï¼Œ
ç»è¿‡æ’æŸ¥å‘ç°æ‰€æœ‰çš„å›¾ç‰‡é“¾æ¥éƒ½ä¸€æ ·é™¤äº†img12.360buyimg.com/n7/jfs/t1/38769/36/65/76968/5cb83ffeEe5900ad2/ee8924e4c5a0fd03.jpg"
è¿™ä¸€æ®µï¼Œæ‰€ä»¥è¿™ä¸€æ®µç”¨.+?ä»£æ›¿ï¼Œè€Œä¸”åƒä¸‡åˆ«è¿™é‡Œä¹Ÿç”¨flags=re.DOTALL
ä¼šå¤šä¸€äº›è«åå…¶å¦™çš„æ•°æ®
å¦‚æœæ˜¯è¿™æ ·
   part2=r'<img width="220" height="220" class="err-product" data-img="1" source-data-lazy-img="//(.+?).jpg" />'
    imagelist=re.compile(part2).findall(result1)
    print(imagelist)

é‚£ç»“æœä¸º
['img14.360buyimg.com/n7/jfs/t1/4528/10/3590/153299/5b997bf5E4a513949/45ab3dd6c35d981b', 'img13.360buyimg.com/n7/jfs/t10675/253/1344769770/66891/92d54ca4/59df2e7fN86c99a27',
è¿™æ²¡æœ‰åé¢çš„.jpgï¼Œ
æ‰€ä»¥æˆ‘ç”¨è¿™æ ·
 part2=r'<img width="220" height="220" class="err-product" data-img="1" source-data-lazy-img="//(.+?\.jpg)" />'
    imagelist=re.compile(part2).findall(result1)
    print(imagelist)
ç»“æœå°±æ˜¯æ­£ç¡®çš„å’¯
    ['img13.360buyimg.com/n7/jfs/t10675/253/1344769770/66891/92d54ca4/59df2e7fN86c99a27.jpg', 'img13.360buyimg.com/n7/jfs/t10690/249/1626659345/69516/b3643998/59e4279aNff3d63ac.jpg',
ä½†æ˜¯éœ€è¦è½¬ä¹‰\å› ä¸ºæœ‰ç‰¹æ®Šå­—ç¬¦.è€Œä¸”éœ€è¦ä¸€ä¸ªï¼ˆï¼‰æŠŠæ­£åˆ™è¡¨è¾¾å¼å’Œ.jpgä¸€èµ·

```
```
çˆ¬å–é“¾æ¥
import re
import urllib.request
def getlink(url):
    headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'}
    req=urllib.request.Request(url=url,headers=headers)
    html=urllib.request.urlopen(req).read().decode()
    print('================================')
    part='(https?://[^\s)";]+\.(\w|/)*)'
    #?æŒ‡httpsæˆ–è€…http
    #[^\s)";]æŒ‡ä¸å¯ä»¥å‡ºç°ç©ºæ ¼\sä¸å‡ºç°åŒå¼•å·ï¼Œåˆ†å·
    link=re.compile(part).findall(html)
    linklist=list(set(link))
    for link in linklist:
        print(link[0])
<!-- ä¿®æ”¹
part='((https|http)://[^\s)";]+\.(\w|/)*)'
åŒ¹é…æ›´å¤šé“¾æ¥ -->


url='http://blog.csdn.net/'
getlink(url)
```
```
çˆ¬å–ç³—äº‹ç™¾ç§‘
import urllib.request
import re
import urllib.request
def getcontent(url,i):
     headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'}
     req=urllib.request.Request(url=url,headers=headers)
     html=urllib.request.urlopen(req).read().decode()
     print('================================')
     part1='<div class="content">(.*?)&lt;/span&gt;'
     content=re.compile(part1,flags=re.DOTALL).findall(html)
     print(content)


for i in range(1,5):
    url='https://www.qiushibaike.com/text/page/'+str(i)
    getcontent(url,i)
```
           BeautifulSoupåº“è§£æhtml/xmlæ–‡æ¡£
é¦–å…ˆBeautifulSoupæœ‰å››ç§è§£æåº“ï¼Œ
1.pythonæ ‡å‡†åº“ ä½¿ç”¨æ–¹æ³•BeautifulSoup(markup,'html.parser')
2.lxml HTMLè§£æå™¨ ä½¿ç”¨æ–¹æ³•BeautifulSoup(markup,'lxml')é€Ÿåº¦å¿«ï¼Œæ–‡æ¡£å®¹é”™èƒ½åŠ›å¼ºï¼Œéœ€è¦å®‰è£…cè¯­è¨€åº“ï¼ˆæ¨èä½¿ç”¨ï¼‰
3 . lxml XMLè§£æå™¨ ä½¿ç”¨æ–¹æ³• BeautifulSoup(markup,'xml;)å”¯ä¸€æ”¯æŒXMLçš„è§£æå™¨ï¼Œéœ€è¦å®‰è£…cè¯­è¨€åº“
4.html5lib ä½¿ç”¨æ–¹æ³•BeautifulSoup(markup,'hml5lib')æœ€åçš„å®¹é”™æ€§ï¼Œä»¥æµè§ˆå™¨çš„æ–¹å¼è§£ææ–‡æ¡£ï¼Œç”Ÿæˆhtml5æ ¼å¼ï¼Œé€Ÿåº¦æ…¢

- åŸºæœ¬ä½¿ç”¨
å»ºè®®ä½¿ç”¨lxml
from bs4 import BeautifulSoup
soup=BeautifulSoup(html,'lxml')
print(soup.prettify())
æ ¼å¼åŒ–å’Œè‡ªåŠ¨è¡¥é½
print(soup.title.string)
è¾“å‡ºtitle çš„å†…å®¹
ä¸‹é¢ä»‹ç»å¸¸ç”¨çš„é€‰æ‹©å™¨
- - -
- ç¬¬ä¸€ç§ ---æ ‡ç­¾é€‰æ‹©å™¨
1 .è·å–æ ‡ç­¾
å¦‚é€‰æ‹©æ ‡ç­¾a li ul class pç­‰æˆ–è€…å­èŠ‚ç‚¹æˆ–è€…çˆ¶èŠ‚ç‚¹ç­‰
```
import requests
from bs4 import BeautifulSoup
r=requests.get('http://python123.io/ws/demo.html')
demo=r.text
soup =BeautifulSoup(demo,'html.parser')
è¿™é‡ŒæŒ‡å®šè§£æå™¨æ˜¯htmlçš„parser
print(soup.prettify())
prettify()å‡½æ•°èƒ½ä¸ºhtmlåšæ¢è¡Œç¬¦ä¹Ÿèƒ½ä¸ºæ¯ä¸€ä¸ªæ ‡ç­¾åšæ ¼å¼
ç»“æœä¸º
<html>
 <head>
  <title>
   This is a python demo page
  </title>
 </head>
 <body>
  <p class="title">
   <b>
    The demo python introduces several python courses.
   </b>
  </p>
  <p class="course">
   Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:
   <a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">
    Basic Python
   </a>
   and
   <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">
    Advanced Python
   </a>
   .
  </p>
 </body>
</html>

```
å¦‚æœæ˜¯æ‰“å¼€æ–‡ä»¶çš„æ ¼å¼å°±ä¸º
soup2=BeautifulSoup(open('D://demo.html'),'html.parser')
```
ä¸‹é¢è¯¦ç»†çœ‹çœ‹BeautifulSoupçš„å¯¹äºhtmlçš„ä¾¿ç­¾
import requests
from bs4 import BeautifulSoup
r=requests.get('http://python123.io/ws/demo.html')
demo=r.text
soup =BeautifulSoup(demo,'html.parser')
print(soup.title)
print('========================')
print(soup.a)
print('========================')
print(soup.a.name)
print('========================')
print(soup.a.parent.name)
print('========================')
print(soup.a.parent.parent.name)
ç»“æœä¸º
<title>This is a python demo page</title>
========================
<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>
========================
a
========================
p
========================
body
```
```
BeautifulSoupå®é™…ä¸Šæ˜¯ä»¥æ ‘å½¢å›¾è§£é‡Š
å¦‚
ä¸‹è¡Œéå†
import requests
from bs4 import BeautifulSoup
r=requests.get('http://python123.io/ws/demo.html')
demo=r.text
soup =BeautifulSoup(demo,'html.parser')
print(soup.head)
ç»“æœä¸º
<head><title>This is a python demo page</title></head>
è‹¥ä¸º
print(soup.head.contents)
ç»“æœä¸º
[<title>This is a python demo page</title>]
å¹³è¡Œéå†
import requests
from bs4 import BeautifulSoup
r=requests.get('http://python123.io/ws/demo.html')
demo=r.text
soup =BeautifulSoup(demo,'html.parser')
print(soup.a)
print('============')
print(soup.a.next_sibling)
print('=========')
print(soup.a.next_sibling.next_sibling)
ç»“æœä¸º
<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>
============
 and 
=========
<a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>
```

 2. è·å–å±æ€§
```
import requests
from bs4 import BeautifulSoup
r=requests.get('http://python123.io/ws/demo.html')
demo=r.text
soup =BeautifulSoup(demo,'html.parser')
tag=soup.a
print(tag.attrs)
ç»“æœä¸º
{'href': 'http://www.icourse163.org/course/BIT-268001', 'class': ['py1'], 'id': 'link1'}
ä»–æ˜¯ä»¥å­—å…¸è¿”å›
æ‰€ä»¥å¯ä»¥è¿™æ ·
print(soup.find_all(attrs={"id":"1"}))

åœ¨lxmlä¹Ÿå¯ä»¥è¿™æ ·
print(soup.find_all(attrs={id="1"}))
å¦‚æœæ˜¯å…³é”®å­—å¾—åŠ _å¦‚print(soup.find_all(attrs={class_="1"}))
ä¹Ÿå¯ä»¥ä½¿ç”¨[]
å¦‚
print(soup.p.attrs['name'])=print(soup.p['name'])

```
bs4çš„ä¿¡æ¯æå–
```
è¯´å®Œäº†å¤§æ¦‚æ–¹å¼ ä¸‹é¢æ¥ä¸€ä¸ªä¾‹å­çˆ¬å–æ‰€æœ‰çš„urlé“¾æ¥
import requests
from bs4 import BeautifulSoup
r=requests.get('http://python123.io/ws/demo.html')
demo=r.text
soup =BeautifulSoup(demo,'html.parser')
for link in soup.find_all('a'):
    print(link.get('href'))
ç»“æœä¸º
http://www.icourse163.org/course/BIT-268001
http://www.icourse163.org/course/BIT-1001870001
å½“ç„¶è¿˜å¯ä»¥ç»“åˆreæ­£åˆ™è¡¨è¾¾å¼
import requests
import re
from bs4 import BeautifulSoup
r=requests.get('http://python123.io/ws/demo.html')
demo=r.text
soup =BeautifulSoup(demo,'html.parser')
for tag in soup.find_all(re.compile('b')):
    print(tag.name)
ç»“æœä¸º
body
b
å®Œç¾é…åˆæ‰¾å‡ºbçš„æ ‡ç­¾åå­—
å½“ç„¶è¿˜å¯ä»¥é€šè¿‡è¿™æ ·æ¥æŸ¥æ‰¾
print(soup.find_all(id='link1'))
ç»“æœä¸º
[<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>]
```
æœ€å bs4çš„ç¼–ç ä¸ºutf8æ‰€ä»¥ä¸python3.ä¸Šå®Œç¾ç¬¦åˆå°±ç®—ä¸­æ–‡ä¹Ÿèƒ½æ— éœ€è§£å†³ç¼–ç é—®é¢˜

 lxmlè§£æå™¨ä¸­ä¹Ÿæ”¯æŒcssé€‰æ‹©å™¨
è¿ç”¨selecté€‰æ‹©â€˜
å¦‚
print(soup.select('ul li'))
ä¸­é—´ä¸€ä¸ªç©ºæ ¼éš”å¼€
è‹¥ä¸€ä¸ªid='list-2'ç›´æ¥å¯ä»¥é€‰æ‹©å†…å®¹ --åŠ #
print(soup.select('#list-2.element'))
ä¹Ÿå¯ä»¥ç”¨ä¸­æ‹¬å·è·å¾—å±æ€§
print(soup.select(ul['id']))
è·å¾—å†…å®¹ä¹Ÿå¯ä»¥ç”¨get_text()
print(soup.select(li.get_text()))

ä¹Ÿå¯ä»¥æ‹–è¿‡è¿™æ ·æŸ¥æ‰¾
```
   soup = BeautifulSoup(response.text, "html.parser")
    student_number = soup.find("span", id="lblStdNum")
    æºä»£ç å¦‚ä¸‹
     <td align="center" style="font-size: 12px;">
                        å­¦&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; å·ï¼š
                    </td>
                    <td>
                        <span id="lblStdNum" style="font-size:12px;"></span>
                    </td>
                    æœ€åå†student['number'] = student_number.text
   
                    å°±èƒ½è¿‡çš„å­¦å·
```
```
                           XPATH
é¦–å…ˆï¼Œpython å…·æœ‰ä¸€äº›æ¯”è¾ƒæµè¡Œçš„è§£æåº“,ä¾‹å¦‚ lxml , ä½¿ç”¨çš„æ˜¯ XPath è¯­æ³•ï¼Œæ˜¯å¤§ä¼—æ™®éè®¤ä¸ºçš„ç½‘é¡µæ–‡æœ¬ä¿¡æ¯æå–çš„çˆ¬è™«åˆ©å™¨ä¹‹ä¸€ã€‚

ä¸€. å…³äº XPath
XPath æ˜¯ XMLè·¯å¾„è¯­è¨€ï¼ˆXML Path Languageï¼‰ï¼Œæ”¯æŒ HTMLï¼Œæ˜¯ä¸€ç§ç”¨æ¥ç¡®å®šXMLæ–‡æ¡£ä¸­æŸéƒ¨åˆ†ä½ç½®çš„è¯­è¨€ã€‚XPathåŸºäºXMLçš„æ ‘çŠ¶ç»“æ„ï¼Œæä¾›åœ¨æ•°æ®ç»“æ„æ ‘ä¸­æŸ¥æ‰¾èŠ‚ç‚¹çš„èƒ½åŠ›ã€‚Xpath å¯ä»¥é€šè¿‡å…ƒç´ å’Œå±æ€§è¿›è¡Œå¯¼èˆªï¼Œç›¸æ¯” æ­£åˆ™è¡¨è¾¾å¼ï¼Œå®ƒåŒæ ·å¯ä»¥åœ¨ XML æ–‡æ¡£ä¸­æŸ¥è¯¢ä¿¡æ¯ï¼Œç”šè‡³ä½¿ç”¨èµ·æ¥æ›´åŠ ç®€å•é«˜æ•ˆã€‚
å¯¹äºä¸€ä¸ª XML æ–‡ä»¶( HTML æ–‡ä»¶å¯ä»¥é€šè¿‡ etree.HTML()æ–¹æ³• è½¬ä¸ºè¿™ç§æ ¼å¼ï¼‰
å¦‚ï¼š# ä½¿ç”¨etreeå¯¹htmlè§£æ
    tree = etree.HTML(pageCode)
    è¡¨è¾¾å¼	æè¿°
nodename	é€‰å–æ­¤èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹ã€‚
/	ä»æ ¹èŠ‚ç‚¹é€‰å–ã€‚
//	ä»åŒ¹é…é€‰æ‹©çš„å½“å‰èŠ‚ç‚¹é€‰æ‹©æ–‡æ¡£ä¸­çš„èŠ‚ç‚¹ï¼Œè€Œä¸è€ƒè™‘å®ƒä»¬çš„ä½ç½®ã€‚
.	é€‰å–å½“å‰èŠ‚ç‚¹ã€‚
..	é€‰å–å½“å‰èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹ã€‚
@	é€‰å–å±æ€§
ï¼
ä¸‹é¢å†æ¥çœ‹çœ‹å‡ ä¸ªå®ä¾‹å¹¶ç†è§£xpath
import requests
from lxml import etree


# è®¾è®¡æ¨¡å¼ --ã€‹é¢å‘å¯¹è±¡ç¼–ç¨‹
class Spider(object):
    def __init__(self):
        # ååçˆ¬è™«æªæ–½ï¼ŒåŠ è¯·æ±‚å¤´éƒ¨ä¿¡æ¯
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36",
            "Referer": "https://www.mzitu.com/xinggan/"
        }


    def xpath_data(self, html):
        # 2. æŠ½å–æƒ³è¦çš„æ•°æ® æ ‡é¢˜ å›¾ç‰‡ xpath
        src_list = html.xpath('//ul[@id="pins"]/li/a/img/@data-original')
        alt_list = html.xpath('//ul[@id="pins"]/li/a/img/@alt')
        for src, alt in zip(src_list, alt_list):
            file_name = alt + ".jpg"
            response = requests.get(src, headers=self.headers)

            # 3. å­˜å‚¨æ•°æ® jpg with open
            try:
                with open('D:/Python_code/pachong/handle/mzitu/'+file_name, "wb") as f:
                    f.write(response.content)
                    print("æ­£åœ¨æŠ“å–å›¾ç‰‡ï¼š" + file_name)
            except:
                print("==========æ–‡ä»¶åæœ‰è¯¯ï¼==========")

    def start_request(self):
        # 1. è·å–æ•´ä½“ç½‘é¡µçš„æ•°æ® requests
        for i in range(1, 5):
            print("==========æ­£åœ¨æŠ“å–%sé¡µ==========" % i)
            response = requests.get("https://www.mzitu.com/page/" + str(i) + "/", headers=self.headers)
            html = etree.HTML(response.content.decode())
            self.xpath_data(html)

spider = Spider()
spider.start_request()
å’Œ
import requests
import threading
from bs4 import BeautifulSoup
import re
from lxml import etree

class tieba(object):
    #1åˆå§‹åŒ–,ä¼ å‚ä½œç”¨ç­‰
    def __init__(self,query_string):
        self.query_string=query_string
        self.url='https://tieba.baidu.com/f'
        self.headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'}

    #2
    def parse_data(self,data,rule):
        html_data=etree.HTML(data)
        data_list=html_data.xpath(rule)
        return data_list


    #3
    def seve_data(self,data,name):
        path='D:/Python_code/pachong/handle/tieba/'+name
        with open (path,'wb') as f:
            f.write(data)
            print("%sçˆ¬å–å®Œæˆ" %name )
    #4
    def params(self):
        para={
            'kw':self.query_string
        }
        return para

    def send_request(self,url,parms={}):
        response=requests.get(url,params=parms,headers=self.headers)
        return response.content


    def run(self):
        tieba_params=self.params()
        datas=self.send_request(self.url,tieba_params)
        #re
        html=datas.decode()
        url_list=re.compile('&lt;a rel="noreferrer" href="(.+)" title.*?&lt;/a&gt;').findall(html)
        # xpath
        # detail_rule="//div[@class='t_con cleafix']/div/div/div/a/@href"
        # url_list=self.parse_data(datas,detail_rule)
        for url in url_list:
            detail_url='https://tieba.baidu.com'+url
            detail_data=self.send_request(detail_url)
            image_url='//img[@class="BDE_Image"]/@src'
            image_url_list=self.parse_data(detail_data,image_url)
            for image_url_1 in image_url_list:
                image_data=self.send_request(image_url_1)
                image_name=image_url_1[-12:]
                self.seve_data(image_data,image_name)



if __name__ == '__main__':
    a=input('è¯·è¾“å…¥ä½ è¦çš„å…³é”®å­—')
    tieba=tieba(a)
    tieba.run()


```
<font face="å®‹ä½“"> <span id="6" color="#456526"><center><b>selenium</b></center></span></font>
- - -
Selenium æ˜¯ä¸ºäº†æµ‹è¯•è€Œå‡ºç”Ÿçš„. ä½†æ˜¯æ²¡æƒ³åˆ°åˆ°äº†çˆ¬è™«çš„å¹´ä»£, å®ƒæ‘‡èº«ä¸€å˜, å˜æˆäº†çˆ¬è™«çš„å¥½å·¥å…·. è®©æˆ‘è¯•ç€ç”¨ä¸€å¥è¯æ¥æ¦‚æ‹¬ Seleninm: å®ƒèƒ½æ§åˆ¶ä½ çš„æµè§ˆå™¨, æœ‰æ¨¡æœ‰æ ·åœ°å­¦äººç±»â€çœ‹â€ç½‘é¡µ.
é‚£ä¹ˆä½ ä»€ä¹ˆæ—¶å€™ä¼šè¦ç”¨åˆ° Selenium å‘¢? å½“ä½ :
å‘ç°ç”¨æ™®é€šæ–¹æ³•çˆ¬ä¸åˆ°æƒ³è¦çš„å†…å®¹
ç½‘ç«™è·Ÿä½ ç©â€æ‰è¿·è—â€, å¤ªå¤š JavaScript å†…å®¹
éœ€è¦åƒäººä¸€æ ·æµè§ˆçš„çˆ¬è™«
æˆ‘æ˜¯é€šè¿‡Anacondaå†™seleniumæ‰€ä»¥æˆ‘åªéœ€è¦! pip install selenium
seleniumçš„ç¯å¢ƒæ­å»ºéœ€è¦æµè§ˆå™¨çš„æ”¯æŒå’Œæµè§ˆå™¨driverçš„æ”¯æŒï¼Œæˆ‘å°±åŒçš„chromeæ‰€ä»¥æˆ‘å¿…é¡»ä¸‹è½½chromedriveræ·»åŠ åˆ°pathé‡Œå³æˆ‘åŠ çš„æ˜¯æˆ‘çš„pythonçš„Scripté‡Œå°±å¯ä»¥äº†
è¯¦æƒ…å‚è€ƒç™¾åº¦python selenium
æˆ–è€…https://selenium-python.readthedocs.io/
ä¸‹é¢è¿è¡Œç¬¬ä¸€ä¸ªselenium
```
from selenium import webdriver
import time
firefox=webdriver.Chrome()
url='http://www.phei.com.cn/'
firefox.get(url)
time.sleep(10)
with open('phei.html','w',encoding='utf8')as f:
    f.write(firefox.page_source)
firefox.quit()
å°±åˆ©ç”¨chromeæ‰“å¼€äº†ä¸€ä¸ªhtml
æœ¬åœ°è¿˜æœ‰ä¸€ä¸ªphei.html
```
```
æŸ¥æ‰¾å•ä¸ªå…ƒç´ 
from selenium import webdriver
browser=webdriver.Chrome()
browser.get('https://www.taobao.com')
input_frist=browser.find_element_by_id('q')
input_second=browser.find_element_by_css_selector('#q')
print(input_frist,input_second)
browser.close()
ç»“æœä¸º
&lt;selenium.webdriver.remote.webelement.WebElement (session="fa3593cb1ecbf4e6e7b70570b9b23b27", element="0.7509356911593199-1")&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session="fa3593cb1ecbf4e6e7b70570b9b23b27", element="0.7509356911593199-1")&gt;
```
```
å¤šä¸ªå…ƒç´ 
from selenium import webdriver
browser=webdriver.Chrome()
browser.get('https://www.taobao.com')
lis=browser.find_elements_by_css_selector('.service-bd li')
print(lis)
browser.close()
é‡Œé¢çš„.service-bd li'æ˜¯æ·˜å®å¯¼èˆªæ¡çš„ä¸€ä¸ªåŒºåŸŸçš„åå­—
ç»“æœä¸º
[&lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-1")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-2")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-3")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-4")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-5")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-6")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-7")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-8")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-9")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-10")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-11")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-12")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-13")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-14")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-15")&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session="38dc04f1721619c75b9957d0bcab5366", element="0.16647184484271205-16")&gt;]
è·å–äº†å¯¼èˆªæ¡çš„ä¸€å¤§æ¨
```
å½“ç„¶è¿˜å¯ä»¥åŠ è½½æ—¶æ§åˆ¶åŠ è½½ æ¯”å¦‚ä¸åŠ è½½å›¾ç‰‡å¦‚
```
# -*- coding:utf-8 -*-
from selenium import webdriver
'''
è®¾ç½®é¡µé¢ä¸åŠ è½½å›¾ç‰‡,è¿™æ ·å¯ä»¥åŠ å¿«é¡µé¢çš„æ¸²æŸ“ï¼Œå‡å°‘çˆ¬è™«çš„ç­‰å¾…æ—¶é—´ï¼Œæå‡çˆ¬å–æ•ˆç‡
å›ºå®šé…ç½®å¦‚ä¸‹ï¼š
'''
chrome_opt = webdriver.ChromeOptions()
prefs = {'profile.managed_default_content_settings.images': 2}
chrome_opt.add_experimental_option('prefs',prefs)
# webdriver.Chrome(executable_path='path')å¯åŠ¨å¤±è´¥çš„è¯,å¯ä»¥æŒ‡å®šChromeDriveré©±åŠ¨çš„ä½ç½®pathè·¯å¾„
browser = webdriver.Chrome(chrome_options=chrome_opt)
# å¯åŠ¨æ·˜å®æµ‹è¯•ç»“æœ
browser.get('https://www.taobao.com')
```
```
ç•Œé¢è·³è½¬
from selenium import webdriver
import time 

browser=webdriver.Chrome()
browser.get('https://www.taobao.com')
input=browser.find_element_by_id('q')
input.send_keys('iphone')
time.sleep(1)
input.clear()
input.send_keys('ipad')
button=browser.find_element_by_class_name('search-button')
button.click()
ç¨‹åºæ‰§è¡Œåæ·˜å®è‡ªåŠ¨ç‚¹å‡»è¿›å…¥äº†ipadçš„é¡µé¢
search-buttonå°±æ˜¯æ·˜å®æºä»£ç é‡Œé¢çš„æŒ‰é’®
click()å°±æ˜¯seleniumæä¾›çš„apiçš„æ–¹æ³•
åŒç†è¿˜æœ‰è®¸å¤š å¦‚click_and_holdç‚¹å‡»ä¹‹åä¸åŠ¨double_clickåŒå‡»ç­‰ç›¸å…³api
```
```
è·å¾—æ ‡ç­¾å†…å®¹ï¼Œæ–‡æœ¬å€¼
from selenium import webdriver
browser=webdriver.Chrome()
url='https://www.zhihu.com/explore'
browser.get(url)
input=browser.find_element_by_class_name('zu-top-add-question')
print(input.text)
print(input.id)
print(input.tag_name)
print(input.location)
è¾“å‡º ä¸º
æé—®
0.7567384595049704-1
button
{'x': 759, 'y': 7}
æºä»£ç æ˜¯<button class="zu-top-add-question" id="zu-top-add-question">æé—®</button>
```
```
å‰è¿›å’Œåé€€
from selenium import webdriver
import time
browser=webdriver.Chrome()
browser.get('https://www.bing.com/')
browser.get('https://www.taobao.com/')
browser.get('https://s.taobao.com/search?q=iphone&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search')
time.sleep(1)
browser.back()
browser.back()
browser.forward()
browser.close()
```
```
é€‰é¡¹å¡
import time 
from selenium import webdriver
browser=webdriver.Chrome()
browser.get('https://www.bing.com/')
browser.execute_script('window.open()')
#è¿ç”¨jsçš„è„šæœ¬å¯ä»¥åˆ‡æ¢å’Œå¤šä¸ªçª—å£
print(browser.window_handles)
browser.switch_to_window(browser.window_handles[1])
browser.get('https://www.taobao.com')
time.sleep(2)
browser.switch_to_window(browser.window_handles[0])
browser.get('https://python.org')
å¤šä¸ªé¡µé¢ï¼Œå¹¶ä¸”å¯ä»¥è‡ªåŠ¨è·³è½¬
```
```
ä¸‹é¢å†æ¥ä¸€ä¸ªseleniumçš„æŠ“å–æ·˜å®ä¿¡æ¯

```

- - -
<font face="å®‹ä½“"> <span id="7" color="#456526"><center>COOKIE</center></span></font>
1.Opener
å½“ä½ è·å–ä¸€ä¸ªURLä½ ä½¿ç”¨ä¸€ä¸ªopenerã€‚åœ¨å‰é¢ï¼Œæˆ‘ä»¬éƒ½æ˜¯ä½¿ç”¨çš„é»˜è®¤çš„openerï¼Œä¹Ÿå°±æ˜¯urlopenã€‚å®ƒæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„openerï¼Œå¯ä»¥ç†è§£æˆopenerçš„ä¸€ä¸ªç‰¹æ®Šå®ä¾‹ï¼Œä¼ å…¥çš„å‚æ•°ä»…ä»…æ˜¯urlï¼Œdataï¼Œtimeoutã€‚

å¦‚æœæˆ‘ä»¬éœ€è¦ç”¨åˆ°Cookieï¼Œåªç”¨è¿™ä¸ªopeneræ˜¯ä¸èƒ½è¾¾åˆ°ç›®çš„çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åˆ›å»ºæ›´ä¸€èˆ¬çš„openeræ¥å®ç°å¯¹Cookieçš„è®¾ç½®ã€‚

2.Cookiejar
cookiejaræ¨¡å—çš„ä¸»è¦ä½œç”¨æ˜¯æä¾›å¯å­˜å‚¨cookieçš„å¯¹è±¡ï¼Œä»¥ä¾¿äºä¸urllibæ¨¡å—é…åˆä½¿ç”¨æ¥è®¿é—®Internetèµ„æºã€‚Cookiejaræ¨¡å—éå¸¸å¼ºå¤§ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨æœ¬æ¨¡å—çš„CookieJarç±»çš„å¯¹è±¡æ¥æ•è·cookieå¹¶åœ¨åç»­è¿æ¥è¯·æ±‚æ—¶é‡æ–°å‘é€ï¼Œæ¯”å¦‚å¯ä»¥å®ç°æ¨¡æ‹Ÿç™»å½•åŠŸèƒ½ã€‚è¯¥æ¨¡å—ä¸»è¦çš„å¯¹è±¡æœ‰CookieJarã€FileCookieJarã€MozillaCookieJarã€LWPCookieJarã€‚

å®ƒä»¬çš„å…³ç³»ï¼šCookieJar â€”-æ´¾ç”Ÿâ€”-&gt;FileCookieJar Â â€”-æ´¾ç”Ÿâ€”â€“&gt;MozillaCookieJarå’ŒLWPCookieJar
ä¸‹é¢æ¥çœ‹çœ‹ä¹¦ä¸­çš„ä»£ç ã€‘
```
import urllib.request
import urllib.parse
import http.cookiejar
url='*********************'
postdata=urllib.paser.urlencode({
    
    'username':'weisuen',
    'password':'aA123456'
}).encode('utf-8')
req=urllib.request.Request(url,postdata)
req.add_header('user-agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36')
#åˆ›å»ºä¸€ä¸ªcookiejarå¯¹è±¡
cjar=http.cookiejar.CookieJar()
#ä½¿ç”¨HTTPCookieProcessoråˆ›å»ºcookieå¤„ç†å™¨ï¼Œå¹¶ä»¥å…¶æ„å»ºå‚æ•°ä½œä¸ºopener
opener =urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cjar))
#å°†openerå®‰è£…ä¸ºå…¨å±€
urllib.request.install_opener(opener)
file=opener.open(req)
data=file.read()
```
```
cookiejarä¸»è¦æ­¥éª¤
1.å¯¼å…¥Cookieå¤„ç†æ¨¡å—http.cookiejar
2.ä½¿ç”¨http.cookiejar.CookieJar()åˆ›å»ºCookieJarå¯¹è±¡
3.ä½¿ç”¨HTTPCookieProcessoråˆ›å»ºcookieå¤„ç†å™¨ï¼Œå¹¶ä»¥å…¶ä¸ºå‚æ•°æ„å»ºopenerå¯¹è±¡
4.åˆ›å»ºå…¨å±€é»˜è®¤çš„openerå¯¹è±¡
```
- - - 
```
1ï¼‰è·å–Cookieä¿å­˜åˆ°å˜é‡
é¦–å…ˆï¼Œæˆ‘ä»¬å…ˆåˆ©ç”¨CookieJarå¯¹è±¡å®ç°è·å–cookieçš„åŠŸèƒ½ï¼Œå­˜å‚¨åˆ°å˜é‡ä¸­ï¼Œå…ˆæ¥æ„Ÿå—ä¸€ä¸‹

from urllib import request
from urllib import parse
from http import cookiejar

#å£°æ˜ä¸€ä¸ªCookieJarå¯¹è±¡å®ä¾‹æ¥ä¿å­˜cookie
cookie = cookiejar.CookieJar()
#åˆ©ç”¨urllibåº“ä¸­çš„requestçš„HTTPCookieProcessorå¯¹è±¡æ¥åˆ›å»ºcookieå¤„ç†å™¨
handler=request.HTTPCookieProcessor(cookie)
#é€šè¿‡handleræ¥æ„å»ºopener
opener = request.build_opener(handler)
#æ­¤å¤„çš„openæ–¹æ³•åŒurllibçš„urlopenæ–¹æ³•ï¼Œä¹Ÿå¯ä»¥ä¼ å…¥request
response = opener.open('http://www.baidu.com')
for item in cookie:
Â Â Â Â print ('Name = '+item.name)
Â Â Â Â print ('Value = '+item.value)



æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸Šæ–¹æ³•å°†cookieä¿å­˜åˆ°å˜é‡ä¸­ï¼Œç„¶åæ‰“å°å‡ºäº†cookieä¸­çš„å€¼ï¼Œè¿è¡Œç»“æœå¦‚ä¸‹


Name = BAIDUID
Value = 5C63AF95C94F5EE96BC89EE5E9CE0188:FG=1
Name = BIDUPSID
Value = 5C63AF95C94F5EE96BC89EE5E9CE0188
Name = H_PS_PSSID
Value = 1431_24557_21123_24022_20928
Name = PSTM
Value = 1508901974
Name = BDSVRTM
Value = 0
Name = BD_HOME
Value = 0





2ï¼‰ä¿å­˜Cookieåˆ°æ–‡ä»¶
åœ¨ä¸Šé¢çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å°†cookieä¿å­˜åˆ°äº†cookieè¿™ä¸ªå˜é‡ä¸­ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å°†cookieä¿å­˜åˆ°æ–‡ä»¶ä¸­è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿè¿™æ—¶ï¼Œæˆ‘ä»¬å°±è¦ç”¨åˆ°

FileCookieJarè¿™ä¸ªå¯¹è±¡äº†ï¼Œå»ºç«‹LWPCookieJarå®ä¾‹ï¼Œå¯ä»¥å­˜Set-Cookie3ç±»å‹çš„æ–‡ä»¶ã€‚è€ŒMozillaCookieJarç±»æ˜¯å­˜ä¸º'.txt'æ ¼å¼çš„æ–‡ä»¶.åœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å®ƒçš„å­ç±»MozillaCookieJaræ¥å®ç°Cookieçš„ä¿å­˜


#!/usr/bin/env/python
#encoding: UTF-8
from urllib import request
from urllib import parse
from http import cookiejar

#è®¾ç½®ä¿å­˜cookieçš„æ–‡ä»¶ï¼ŒåŒçº§ç›®å½•ä¸‹çš„cookie.txt
filename = 'cookie.txt'
#å£°æ˜ä¸€ä¸ªMozillaCookieJarå¯¹è±¡å®ä¾‹æ¥ä¿å­˜cookieï¼Œä¹‹åå†™å…¥æ–‡ä»¶
cookie = cookiejar.MozillaCookieJar(filename)
#åˆ©ç”¨urllibåº“çš„HTTPCookieProcessorå¯¹è±¡æ¥åˆ›å»ºcookieå¤„ç†å™¨
handler = request.HTTPCookieProcessor(cookie)
#é€šè¿‡handleræ¥æ„å»ºopener
opener = request.build_opener(handler)
#åˆ›å»ºä¸€ä¸ªè¯·æ±‚ï¼ŒåŸç†åŒurllib2çš„urlopen
response = opener.open("http://www.baidu.com")
#ä¿å­˜cookieåˆ°æ–‡ä»¶
cookie.save(ignore_discard=True, ignore_expires=True)


å…³äºæœ€åsaveæ–¹æ³•çš„ä¸¤ä¸ªå‚æ•°åœ¨æ­¤è¯´æ˜ä¸€ä¸‹ï¼š

å®˜æ–¹è§£é‡Šå¦‚ä¸‹ï¼š

ignore_discard: save even cookies set to be discarded.Â 

ignore_expires: save even cookies that have expiredThe file is overwritten if it already exists

ç”±æ­¤å¯è§ï¼Œignore_discardçš„æ„æ€æ˜¯å³ä½¿cookieså°†è¢«ä¸¢å¼ƒä¹Ÿå°†å®ƒä¿å­˜ä¸‹æ¥ï¼Œignore_expiresçš„æ„æ€æ˜¯å¦‚æœåœ¨è¯¥æ–‡ä»¶ä¸­cookieså·²ç»å­˜åœ¨ï¼Œåˆ™è¦†ç›–åŸæ–‡ä»¶å†™å…¥ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è¿™ä¸¤ä¸ªå…¨éƒ¨è®¾ç½®ä¸ºTrueã€‚è¿è¡Œä¹‹åï¼Œcookieså°†è¢«ä¿å­˜åˆ°cookie.txtæ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬æŸ¥çœ‹ä¸€ä¸‹å†…å®¹ï¼Œé™„å›¾å¦‚ä¸‹



ä½¿ç”¨LWPCookieJaræ¥ä¿å­˜cookie:

filename = 'cookie'

cookie = cookiejar.LWPCookieJar(filename)

ä½¿ç”¨çš„æ—¶å€™ç›´æ¥loadã€‚

Â 3ï¼‰ä»æ–‡ä»¶ä¸­è·å–Cookieå¹¶è®¿é—®
é‚£ä¹ˆæˆ‘ä»¬å·²ç»åšåˆ°æŠŠCookieä¿å­˜åˆ°æ–‡ä»¶ä¸­äº†ï¼Œå¦‚æœä»¥åæƒ³ä½¿ç”¨ï¼Œå¯ä»¥åˆ©ç”¨ä¸‹é¢çš„æ–¹æ³•æ¥è¯»å–cookieå¹¶è®¿é—®ç½‘ç«™ï¼Œæ„Ÿå—ä¸€ä¸‹

#!/usr/bin/env/python
#encoding: UTF-8

from urllib import request
from urllib import parse
from http import cookiejar

cookie = cookiejar.MozillaCookieJar()
cookie.load('cookie.txt', ignore_discard=True, ignore_expires=True)
req= request.Request('http://www.baidu.com')
opener = request.build_opener(request.HTTPCookieProcessor(cookie))
response = opener.open(req)
print(response.read())


è®¾æƒ³ï¼Œå¦‚æœæˆ‘ä»¬çš„ cookie.txt æ–‡ä»¶ä¸­ä¿å­˜çš„æ˜¯æŸä¸ªäººç™»å½•ç™¾åº¦çš„cookieï¼Œé‚£ä¹ˆæˆ‘ä»¬æå–å‡ºè¿™ä¸ªcookieæ–‡ä»¶å†…å®¹ï¼Œå°±å¯ä»¥ç”¨ä»¥ä¸Šæ–¹æ³•æ¨¡æ‹Ÿè¿™ä¸ªäººçš„è´¦å·ç™»å½•ç™¾åº¦ã€‚

Â 4ï¼‰åˆ©ç”¨cookieæ¨¡æ‹Ÿç½‘ç«™ç™»å½•
ä¸‹é¢æˆ‘ä»¬ä»¥çŸ¥ä¹ä¸ºä¾‹ï¼Œåˆ©ç”¨cookieå®ç°æ¨¡æ‹Ÿç™»å½•ï¼Œæ¥æ„Ÿå—ä¸€ä¸‹cookieå¤§æ³•å§ï¼

#!/usr/bin/env/python
#encoding: UTF-8
 
import re
import requests
import http.cookiejar
from PIL import Image
import time
import json
 
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 '
                         '(KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36',
           "Host": "www.zhihu.com",
           "Referer": "https://www.zhihu.com/",
           }
# å»ºç«‹ä¸€ä¸ªä¼šè¯ï¼Œå¯ä»¥æŠŠåŒä¸€ç”¨æˆ·çš„ä¸åŒè¯·æ±‚è”ç³»èµ·æ¥ï¼›ç›´åˆ°ä¼šè¯ç»“æŸéƒ½ä¼šè‡ªåŠ¨å¤„ç†cookies
session = requests.Session()
# å»ºç«‹LWPCookieJarå®ä¾‹ï¼Œå¯ä»¥å­˜Set-Cookie3ç±»å‹çš„æ–‡ä»¶ã€‚
# è€ŒMozillaCookieJarç±»æ˜¯å­˜ä¸º'/.txt'æ ¼å¼çš„æ–‡ä»¶
session.cookies = http.cookiejar.LWPCookieJar("cookie")
# è‹¥æœ¬åœ°æœ‰cookieåˆ™ä¸ç”¨å†postæ•°æ®äº†
try:
    session.cookies.load(ignore_discard=True)
except IOError:
    print('CookieæœªåŠ è½½ï¼')
 
 
def get_xsrf():
    """
    è·å–å‚æ•°_xsrf
    """
    response = session.get('https://www.zhihu.com', headers=headers)
    html = response.text
    get_xsrf_pattern = re.compile(r'&lt;input type="hidden" name="_xsrf" value="(.*?)"')
    _xsrf = re.findall(get_xsrf_pattern, html)[0]
    return _xsrf
 
 
def get_captcha():
    """
    è·å–éªŒè¯ç æœ¬åœ°æ˜¾ç¤º
    è¿”å›ä½ è¾“å…¥çš„éªŒè¯ç 
    """
    t = str(int(time.time() * 1000))
    captcha_url = 'http://www.zhihu.com/captcha.gif?r=' + t + "&amp;type=login"
    response = session.get(captcha_url, headers=headers)
    with open('cptcha.gif', 'wb') as f:
        f.write(response.content)
    # Pillowæ˜¾ç¤ºéªŒè¯ç 
    im = Image.open('cptcha.gif')
    im.show()
    captcha = input('æœ¬æ¬¡ç™»å½•éœ€è¦è¾“å…¥éªŒè¯ç ï¼š ')
    return captcha
 
 
def login(username, password):
    """
    è¾“å…¥è‡ªå·±çš„è´¦å·å¯†ç ï¼Œæ¨¡æ‹Ÿç™»å½•çŸ¥ä¹
    """
    # æ£€æµ‹åˆ°11ä½æ•°å­—åˆ™æ˜¯æ‰‹æœºç™»å½•
    if re.match(r'\d{11}$', username):
        url = 'http://www.zhihu.com/login/phone_num'
        data = {'_xsrf': get_xsrf(),
                'password': password,
                'remember_me': 'true',
                'phone_num': username
                }
    else:
        url = 'https://www.zhihu.com/login/email'
        data = {'_xsrf': get_xsrf(),
                'password': password,
                'remember_me': 'true',
                'email': username
                }
    # è‹¥ä¸ç”¨éªŒè¯ç ï¼Œç›´æ¥ç™»å½•
    result = session.post(url, data=data, headers=headers)
    # æ‰“å°è¿”å›çš„å“åº”ï¼Œr = 1ä»£è¡¨å“åº”å¤±è´¥ï¼Œmsgé‡Œæ˜¯å¤±è´¥çš„åŸå› 
    # loadså¯ä»¥ååºåˆ—åŒ–å†…ç½®æ•°æ®ç±»å‹ï¼Œè€Œloadå¯ä»¥ä»æ–‡ä»¶è¯»å–
    if (json.loads(result.text))["r"] == 1:
        # è¦ç”¨éªŒè¯ç ï¼Œpoståç™»å½•
        data['captcha'] = get_captcha()
        result = session.post(url, data=data, headers=headers)
        print((json.loads(result.text))['msg'])
        # ä¿å­˜cookieåˆ°æœ¬åœ°
    session.cookies.save(ignore_discard=True, ignore_expires=True)
 
 
def isLogin():
    # é€šè¿‡æŸ¥çœ‹ç”¨æˆ·ä¸ªäººä¿¡æ¯æ¥åˆ¤æ–­æ˜¯å¦å·²ç»ç™»å½•
    url = "https://www.zhihu.com/settings/profile"
    # ç¦æ­¢é‡å®šå‘ï¼Œå¦åˆ™ç™»å½•å¤±è´¥é‡å®šå‘åˆ°é¦–é¡µä¹Ÿæ˜¯å“åº”200
    login_code = session.get(url, headers=headers, allow_redirects=False).status_code
    if login_code == 200:
        return True
    else:
        return False
 
 
if __name__ == '__main__':
    if isLogin():
        print('æ‚¨å·²ç»ç™»å½•')
    else:
        account = input('è¾“å…¥è´¦å·ï¼š')
        secret = input('è¾“å…¥å¯†ç ï¼š')
        login(account, secret)


ä»¥ä¸Šç¨‹åºçš„åŸç†å¦‚ä¸‹

åˆ›å»ºä¸€ä¸ªå¸¦æœ‰cookieçš„openerï¼Œåœ¨è®¿é—®ç™»å½•çš„URLæ—¶ï¼Œå°†ç™»å½•åçš„cookieä¿å­˜ä¸‹æ¥ï¼Œç„¶ååˆ©ç”¨è¿™ä¸ªcookieæ¥è®¿é—®å…¶ä»–ç½‘å€ã€‚

å¦‚ç™»å½•ä¹‹åæ‰èƒ½æŸ¥çœ‹çš„æˆç»©æŸ¥è¯¢å‘€ï¼Œæœ¬å­¦æœŸè¯¾è¡¨å‘€ç­‰ç­‰ç½‘å€ï¼Œæ¨¡æ‹Ÿç™»å½•å°±è¿™ä¹ˆå®ç°å•¦ï¼Œæ˜¯ä¸æ˜¯å¾ˆé…·ç‚«ï¼Ÿ


```
- - - 
<font face="å®‹ä½“"> <span id="8" color="#456526"><center>mongoDB</center></span></font>
åœ¨è¿™ä¹‹å‰ï¼Œé€šè¿‡Djangoå­¦ä¹ äº†mysqlä¹Ÿå¯¹ä»–æœ‰äº†ä¸€ç‚¹äº†è§£ï¼Œä¸‹é¢æ¥çœ‹çœ‹çˆ¬è™«ç»å¸¸ç”¨çš„æ•°æ®åº“mongoDB
é¦–å…ˆæ˜¯å®‰è£…mongoDBï¼Œç›´æ¥è¿›å…¥å®˜ç½‘ï¼Œç„¶åä¸‹è½½ç›¸åº”ç‰ˆæœ¬çš„commiunity ç¤¾åŒºç‰ˆï¼Œç„¶åä¸€è·¯installå’Œnext åˆ‡è®°ï¼Œä½ç½®ä¸€å®šåœ¨æ ¹ç›®å½•ä¸‹ï¼Œä¸ç„¶æœ€åçš„æ—¶å€™å¦‚æœå‡ºç°é”™è¯¯ç„¶åå°±ä¼šæœ‰ä¸‰ä¸ªæ–‡ä»¶åˆ†åˆ«æ˜¯bin data logï¼Œè€Œä¸”æœ€å¥½æŠŠmongo dbçš„compassä¸€èµ·ä¸‹è½½å¥½ï¼Œé»˜è®¤çš„çš„æ˜¯http://localhost:27017ä¸‹è½½å®Œæˆåæ‰“å¼€ç›®å½•å°±æœ‰ä¸€ä¸ªbinå’Œlog,dataäº†ï¼Œå¯ä»¥åœ¨biné‡Œé¢cmdæ‰“å¼€å‘½ä»¤è¡Œè¾“å…¥mongoå°±è¿›å…¥äº†mongodbçš„æ•°æ®åº“ï¼Œå¦‚æœæ²¡æœ‰è¿›å…¥åˆ™mongod --dbpath ä½ç½® å°±OKäº†ï¼Œåœ¨æ•°æ®åº“é‡Œé¢è¾“å…¥dbå°±å¯ä»¥çœ‹è§æœ‰ä¸€ä¸ªtestå°±å¯ä»¥å¯¹å®ƒè¿›è¡Œå¢åˆ é™¤ä¹‹åˆ—çš„,å°±å¼€å¯äº†mongodb çš„å­¦ä¹ ä¹‹è·¯

è€Œæˆ‘çš„ç”µè„‘è¿æ¥mongodbä¸¤ç§æ–¹æ³• ï¼Œç¬¬ä¸€ç§ç›´æ¥ç”¨å‘½ä»¤mongod--dbpath E:\dataæˆ–è€…ç›´æ¥æœç´¢æœåŠ¡æ‰¾åˆ°mondodb server æ‰“å¼€å³å¯ï¼Œå¦‚æœè¦æ‰“å¼€mysqlå‚è€ƒDjango
db.test.insert(('a':'b'))ç­‰åŸºæœ¬æ“ä½œå¿˜äº†è‡ªå·±ç™¾åº¦
1.å®‰è£…ä¹Ÿå¯ä»¥sudo apt - get install mongodb - server
æœ€å¥½è¿˜æ˜¯å®˜ç½‘ï¼Œæ–°ç‰ˆæœ¬é™¤äº†åè‡ªåŠ¨é…ç½®å¥½äº†ï¼Œåªéœ€è¦ä¸‹è½½å°±è¡Œäº†
2.æŒ‡å®šæ•°æ®åº“ä½ç½®å³dataï¼Œè¿™ä¸ªä¹Ÿæ˜¯é—ªé€€åè§£å†³åŠæ³•ä¹‹ä¸€åœ¨biné‡Œé¢cmdæ‰§è¡Œ
mongod --dbpath ä½ç½® å°±OKäº†
3. pythonä¸ºäº†è¿æ¥mongo DB éœ€è¦pymongo, pip install pymongo
4.import pymongo
client=pymongo.MongoClient('127.0.0.1',27017)è¿æ¥å®¢æˆ·ç«¯
å½“ç„¶ä¹Ÿå¯ä»¥client=pymongo.MongoClient('localhost',port=27017)
5.db=client['shujuku_name']åˆ›å»ºä¸€ä¸ªæ•°æ®åº“ï¼Œæœ‰å°±ç”¨è¿™ä¸ªæ•°æ®åº“ï¼Œæ²¡æœ‰å°±è‡ªåŠ¨åˆ›å»º
db['biaodan_name'].insert(result)

- - - 
<font face="å®‹ä½“"> <span id="9" color="#456526"><center>pyè½¬exe</center></span></font>
1.å‡†å¤‡å¥½éœ€è¦è½¬æ¢çš„pyæ–‡ä»¶å’Œä¸€å¼ ç”¨äºåšå›¾æ ‡çš„ç…§ç‰‡ï¼Œå°†ä»–ä»¬å­˜æ”¾äºåŒä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œæ–‡ä»¶çš„è·¯å¾„å…¨éƒ¨ä¸ºè‹±æ–‡è·¯å¾„
2.åœ¨ç½‘ä¸Šå°†å›¾æ ‡è£…æ¢ä¸º.icoæ ¼å¼
3.åˆ©ç”¨å‘½ä»¤çª—å£å®‰è£…pyinstalleræ’ä»¶ pip install 
4.å°†å‘½ä»¤çª—å£è·¯å¾„åˆ‡æ¢åˆ°éœ€è¦å¤„ç†çš„pyæ–‡ä»¶çš„è·¯å¾„ï¼Œä½¿ç”¨cdå‘½ä»¤æ¥å®Œæˆ
5.æ‰§è¡Œå‘½ä»¤ pyinstaller -F -i 1.ico 2.py(å³icoå’Œpyçš„åå­—)
6.æ‰§è¡Œå®Œå‘½ä»¤åï¼Œéœ€è¦çš„exeæ–‡ä»¶å°±åœ¨distæ–‡ä»¶å¤¹ä¸­
- - -
<font face="å®‹ä½“"> <span id="9" color="#456526"><center>redis</center></span></font>
é¦–å…ˆä¸‹è½½redisç›´æ¥æœç´¢rediså®‰è£…ï¼Œæœ‰ä¸€ä¸ªèœé¸Ÿæ•™ç¨‹è¿˜å¯ä»¥ï¼Œç‚¹å‡»è¿›å…¥ä¸€ä¸ªgithubï¼Œhttps://github.com/MSOpenTech/redis/releasesã€‚æ‰¾åˆ°æƒ³è¦çš„å‘è¡Œç‰ˆæœ¬ï¼Œå®‰è£…ä¸‹è½½å³å¯ï¼Œç„¶ååœ¨githubæœç´¢redis desktopå®‰è£…å¯è§†åŒ–ç•Œé¢ï¼Œå¼ºè°ƒä¸‹ï¼Œæˆ‘çš„éƒ½æ˜¯åœ¨Eç›˜çš„æ ¹ç›®å½•ã€‚
- - - 
<font face="å®‹ä½“"> <span id="10" color="#456526"><center>æ¡ˆåˆ—</center></span></font>
```
äº¬ä¸œå›¾ç‰‡
import urllib.request
import re
from bs4 import BeautifulSoup


def crawl(url,page):
    headers={'url-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'}
    req=urllib.request.Request(url=url,headers=headers)
    html=urllib.request.urlopen(req).read()
    html1=html.decode() #æ ¼å¼å˜å¾—æ ‡å‡†åŒ–ã€‚è§£ç 
    part1 = r'&lt;div id="J_goodsList" .+?<div class="m-aside">'
    result1 = re.compile(part1,flags=re.DOTALL).findall(html1)
    result1=result1[0] #ä½¿æ•°æ®å˜å¾—å·¥æ•´è€Œä¸”æ ¼å¼åŒ–äº† ä¹ˆä»¥å¶äº†/nç­‰é‡å¤
    part2=r'<img width="220" height="220" class="err-product" data-img="1" source-data-lazy-img="//(.+?\.jpg)" />'
    imagelist=re.compile(part2).findall(result1)
    x=1
    for imageurl in imagelist:
        imagename="D:/python_code/pachong/handle/jd_image/"+str(page)+str(x)+'.jpg'
        imageurl='http://'+imageurl
        try:
            urllib.request.urlretrieve(imageurl,filename=imagename)
        except urllib.error.URLError as e:
            if hasattr(e,'code'):
                x+=1
            if hasattr(e,'reason'):
                x+=1
        x+=1

for i in range(1,5):
    url='https://search.jd.com/Search?keyword=iphonex&amp;enc=utf-8&amp;qrst=1&amp;rt=1&amp;stop=1&amp;vt=2&amp;suggest=1.his.0.0&amp;page='+str(i)
    crawl(url,i)
    print('ç¬¬içˆ¬å–æˆåŠŸ')
ä»Šæ—¥å¤´æ¡
import requests
import re
import json
from urllib.parse import urlencode
from requests.exceptions import RequestException
import pymongo
import os
from hashlib import md5

MONG_DB='toutiao'
MONG_TABLE='toutiao'
client=pymongo.MongoClient('127.0.0.1',27017)
db=client[MONG_DB]


def seve_mongo(result):
    if db[MONG_TABLE].insert(result):  #ä¸€å®šæ˜¯[]
        print('å­˜è“„æˆåŠŸ',result)
        return True
    return False


def get_index_page(offset,keyworld):
    data={
        'aid': '24',
        'app_name': 'web_search',
        'offset':offset,
        'format': 'json',
        'keyword': keyworld,
        'autoload': 'true',
        'count': '20',
        'en_qc':'1',
        'cur_tab':'1',
        'from': 'search_tab',
        'pd': 'synthesis',
    }
    headers={
        'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'
    }
    cookies={
        'cookie':'tt_webid=6691096928111166988; WEATHER_CITY=%E5%8C%97%E4%BA%AC; tt_webid=6691096928111166988; UM_distinctid=16ab99fb45d16e-04be26d83dc51d-6353160-144000-16ab99fb45e30f; csrftoken=3859f31127aece7bfebdd8efd257d464; s_v_web_id=dc502212a4a9466637eee610d6bd51b6; __tasessionId=jvaqvqe0u1557902021235; CNZZDATA1259612802=1956023403-1557890949-https%253A%252F%252Fcn.bing.com%252F%7C1557901749'
    }
    url='https://www.toutiao.com/api/search/content/?'+urlencode(data)
    #å®ƒå¯ä»¥æŠŠå­—å…¸ç±»å‹è½¬æ¢ä¸ºurlæ ¼å¼
    response=requests.get(url,headers=headers,cookies=cookies)
    try:
      if response.status_code == 200:
          return response.text
      else:
           None
    except RequestException:
        print('è¯·æ±‚ç½‘é¡µå‡ºé”™')
        return None


def parse_page_index_url(html):
    data=json.loads(html)
    #json.loadå°†å­—ç¬¦ä¸²å˜æˆä¸€ä¸ªå¯¹è±¡json
    if data and 'data' in data.keys():  #data.keysè¿”å›çš„å°±æ˜¯é”®å
        for item in data.get('data'):
            yield item.get('article_url') #yield ç”Ÿæˆå™¨,yieldï¼šå¯ä»¥çœ‹ä¸ºreturnï¼Œä½†æ˜¯è¿”å›çš„ä¸æ˜¯ä¸€ä¸ªå‡½æ•°çš„è¾“å‡ºï¼Œæ˜¯ä¸€ä¸ªç”Ÿæˆå™¨çš„ç»“æœã€‚å½“ç¨‹åºé‡åˆ°yieldåï¼Œåœæ­¢æ‰§è¡Œå¹¶è¿”å›ï¼Œå½“å†æ¬¡è°ƒç”¨æ—¶ï¼Œä¼šåœ¨åœæ­¢çš„åœ°æ–¹ç»§ç»­æ‰§è¡Œã€‚ä¸€ä¸ªæ–¹æ³•ç”¨æ¥yieldï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ã€‚


def get_page_detail(url):
    headers = {
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'
    }
    cookies = {
        'cookie': 'tt_webid=6691096928111166988; WEATHER_CITY=%E5%8C%97%E4%BA%AC; tt_webid=6691096928111166988; UM_distinctid=16ab99fb45d16e-04be26d83dc51d-6353160-144000-16ab99fb45e30f; csrftoken=3859f31127aece7bfebdd8efd257d464; s_v_web_id=dc502212a4a9466637eee610d6bd51b6; __tasessionId=jvaqvqe0u1557902021235; CNZZDATA1259612802=1956023403-1557890949-https%253A%252F%252Fcn.bing.com%252F%7C1557901749'
    }
    try:
        response=requests.get(url,headers=headers,cookies=cookies)
        if response.status_code == 200:
            return response.text
        else:
           None
    except RequestException:
        print('è¯·æ±‚è¯¦æƒ…é¡µå‡ºé”™',url)
        return None


def get_page_detail_url(html_detail):
        part=r'img src&#x3D;&quot;(.*?)&quot; img_width'
        result=re.compile(part,flags=re.DOTALL).findall(str(html_detail))
        part1 = r"title: '(.*?)'"
        result1 = re.compile(part1, flags=re.DOTALL).findall(str(html_detail))
        return {
            'title':result1,
            'url':result,
        }


def get_page_detail_title(html_detail):
    part=r"title: '(.*?)'"
    result = re.compile(part, flags=re.DOTALL).findall(str(html_detail))
    return result


def download_image(url):
    print('å½“å‰æ­£åœ¨ä¸‹è½½',url)
    try:
        response = requests.get(url)
        if response.status_code==200:
            save_image(response.content)
        return None
    except RequestException:
        print('è¯·æ±‚å›¾ç‰‡å‡ºé”™',url)
        return None
def save_image(content):
    file_path='{0}/{1}.{2}'.format(os.getcwd(),md5(content).hexdigest(),'jpg')
    if not os.path.exists(file_path):
        with open(file_path,'wb')as f:
            f.write(content)
            f.close()

def main():
    # a=input('è¯·è¾“å…¥é¡µé¢æ•°:')
    # b=input('è¯·è¾“å…¥å…³é”®å­—:')
    # html=get_index_page(a,str(b))
    html = get_index_page(0, 'è¡—æ‹')
    for url in parse_page_index_url(html):
        html_detail=get_page_detail(url)
        result=get_page_detail_url(html_detail)
        seve_mongo(result)


if __name__ == '__main__':
    main()
çŒ«çœ¼ç”µå½±
import requests
import re
from requests.exceptions import RequestException

def get_one_page(url):
    try:
        response = requests.get(url)
        if response.status_code==200:
            return response.text
        return None
    except RequestException:
        return None

def parse_one_page(html):
    pattern = re.compile(r'<dd>.*?<i class="board-index board-index-.*?">(\d+)</i>.*?&lt;a href="(.*?)".*?title="(.*?)".*?&lt;img data-src="(.*?)".*?<p class="star">(.*?)</p>.*?<p class="releasetime">(.*?)</p>.*?<i class="integer">(\d+)',flags=re.DOTALL)
    # è¿™æ ·å°±ä¸ä¼šå‡ºç»“æœpattern = re.compile(r'<dd>.*?<i class="board-index board-index-.*?">(\d+)</i>.*?&lt;a href="(.*?)".*?title="(.*?)".*?&lt;img data-src="(.*?)".*?<p class="star">(.*?)</p>.*?<p class="releasetime">(.*?)</p>.*?<i class="integer">(\d+)</i>',flags=re.DOTALL)
    items=re.findall(pattern,html)
    print(items)

def main():
    url='https://maoyan.com/board/4?offset=0'
    html=get_one_page(url)
    print(html)
    parse_one_page(html)

if __name__=='__main__':
    main()
ç¾å¥³å›¾
import requests
from lxml import etree


# è®¾è®¡æ¨¡å¼ --ã€‹é¢å‘å¯¹è±¡ç¼–ç¨‹
class Spider(object):
    def __init__(self):
        # ååçˆ¬è™«æªæ–½ï¼ŒåŠ è¯·æ±‚å¤´éƒ¨ä¿¡æ¯
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36",
            "Referer": "https://www.mzitu.com/xinggan/"
        }


    def xpath_data(self, html):
        # 2. æŠ½å–æƒ³è¦çš„æ•°æ® æ ‡é¢˜ å›¾ç‰‡ xpath
        src_list = html.xpath('//ul[@id="pins"]/li/a/img/@data-original')
        alt_list = html.xpath('//ul[@id="pins"]/li/a/img/@alt')
        for src, alt in zip(src_list, alt_list):
            file_name = alt + ".jpg"
            response = requests.get(src, headers=self.headers)

            # 3. å­˜å‚¨æ•°æ® jpg with open
            try:
                with open('D:/Python_code/pachong/handle/mzitu/'+file_name, "wb") as f:
                    f.write(response.content)
                    print("æ­£åœ¨æŠ“å–å›¾ç‰‡ï¼š" + file_name)
            except:
                print("==========æ–‡ä»¶åæœ‰è¯¯ï¼==========")

    def start_request(self):
        # 1. è·å–æ•´ä½“ç½‘é¡µçš„æ•°æ® requests
        for i in range(1, 5):
            print("==========æ­£åœ¨æŠ“å–%sé¡µ==========" % i)
            response = requests.get("https://www.mzitu.com/page/" + str(i) + "/", headers=self.headers)
            html = etree.HTML(response.content.decode())
            self.xpath_data(html)

spider = Spider()
spider.start_request()
æ·˜å®
from selenium import webdriver
import requests
#ä¸‹é¢ä¸‰ä¸ªå¤åˆ¶python seleniumçš„wait
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

browser =webdriver.Chrome()
def search():
    try:
        browser.get('https://www.taobao.com')
        # ä»¥ä¸‹ä»£ç ä¹Ÿæ˜¯å¤åˆ¶è¿‡æ¥,å°†driveræ”¹ä¸ºbrowserç­‰æ”¹åŠ¨éƒ½åœ¨é¡µé¢æœ‰
        # è€Œ(By.CSS_SELECTOR, "#q")è¿™ä¸ªåˆ™æ˜¯ç›´æ¥åœ¨æ·˜å®ç½‘é¡µé€‰æ‹©è¯¥å…ƒç´ ï¼Œç„¶åç‚¹å‡»copy,copy selectorå°±æ˜¯ä»–ä»¬äº†å¦‚æœç´¢æ˜¯#qï¼Œç‚¹å‡»æ˜¯#J_TSearchForm &gt; div.search-button &gt; button
        # element = WebDriverWait(driver, 10).until(
        #     EC.presence_of_element_located((By.ID, "myDynamicElement"))
        input = WebDriverWait(browser, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#q"))
        )
        submit=WebDriverWait(browser, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR,"#J_TSearchForm &gt; div.search-button &gt; button"))
        )
        #ä¸‹é¢å°±æ˜¯æ¨¡æ‹Ÿæ“ä½œäº†,ä¹Ÿåœ¨é¡µé¢é‡Œé¢æœ‰å¦‚send_keyså°±åœ¨7.2ä¸­
        input.send_keys('ç¾é£Ÿ')
        submit.click()
        total=WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,"#J_relative &gt; div.sort-row &gt; div &gt; div.pager &gt; ul &gt; li:nth-child(2)")))
        return total.text
    except TimeoutError:
        return search()


def main():
    total=search()



if __name__=='__main__':
    main()
è´´å§
import requests
import threading
from bs4 import BeautifulSoup
import re
from lxml import etree

class tieba(object):
    #1åˆå§‹åŒ–,ä¼ å‚ä½œç”¨ç­‰
    def __init__(self,query_string):
        self.query_string=query_string
        self.url='https://tieba.baidu.com/f'
        self.headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'}

    #2
    def parse_data(self,data,rule):
        html_data=etree.HTML(data)
        data_list=html_data.xpath(rule)
        return data_list


    #3
    def seve_data(self,data,name):
        path='D:/Python_code/pachong/handle/tieba/'+name
        with open (path,'wb') as f:
            f.write(data)
            print("%sçˆ¬å–å®Œæˆ" %name )
    #4
    def params(self):
        para={
            'kw':self.query_string
        }
        return para

    def send_request(self,url,parms={}):
        response=requests.get(url,params=parms,headers=self.headers)
        return response.content


    def run(self):
        tieba_params=self.params()
        datas=self.send_request(self.url,tieba_params)
        #re
        html=datas.decode()
        url_list=re.compile('&lt;a rel="noreferrer" href="(.+)" title.*?&lt;/a&gt;').findall(html)
        # xpath
        # detail_rule="//div[@class='t_con cleafix']/div/div/div/a/@href"
        # url_list=self.parse_data(datas,detail_rule)
        for url in url_list:
            detail_url='https://tieba.baidu.com'+url
            detail_data=self.send_request(detail_url)
            image_url='//img[@class="BDE_Image"]/@src'
            image_url_list=self.parse_data(detail_data,image_url)
            for image_url_1 in image_url_list:
                image_data=self.send_request(image_url_1)
                image_name=image_url_1[-12:]
                self.seve_data(image_data,image_name)



if __name__ == '__main__':
    a=input('è¯·è¾“å…¥ä½ è¦çš„å…³é”®å­—')
    tieba=tieba(a)
    tieba.run()
vipç”µå½±
import tkinter
import tkinter.messagebox
import webbrowser

def Button():
    a = 'http://www.82190555.com/video.php?url=' if varRadio.get() else 'http://jx.598110.com/?url='
    b = entry_movie_link.get()
    webbrowser.open(a+b)
def qk():
    entry_movie_link.delete(0,'end')

def openaqy():
    webbrowser.open('http://www.iqiyi.com')

def opentx():
    webbrowser.open('http://v.qq.com')

def openyq():
    webbrowser.open('http://www.youku.com/')

def about():
    abc='''
    ç»è¿‡æµ‹è¯•çˆ±å¥‡è‰ºã€ä¼˜é…·ã€è…¾è®¯çš„VIPè§†é¢‘å¯ä»¥æ’­æ”¾
    é“¾æ¥æ ¼å¼ä¸º
    http://www.iqiyi.com/v_19rrb2u62s.html?fc=82992814760eeac6
    é€šé“ å¯èƒ½ç”±äºå˜æ¢æ²¡æ³•æ’­å‡ºæ—¶è¯·åˆ‡æ¢é€šé“
    '''
    tkinter.messagebox.showinfo(title='å¸®åŠ©æ–‡ä»¶', message=abc)
def zzxx():
    msg='''
            yj  666
    '''
    tkinter.messagebox.showinfo(title='è”ç³»æ–¹å¼', message=msg)
if __name__ == '__main__':
    root=tkinter.Tk()
    root.title('yj666')
    root['width']=500
    root['height']=300

    menubar = tkinter.Menu(root)
    helpmenu = tkinter.Menu(menubar, tearoff=0)
    helpmenu.add_command(label='å¸®åŠ©æ–‡æ¡£', command=about)
    helpmenu.add_command(label='ä½œè€…ä¿¡æ¯', command=zzxx)
    menubar.add_cascade(label='å¸®åŠ©(H)', menu=helpmenu)
    root.config(menu=menubar)

    varentry1= tkinter.StringVar(value='')
    lab_movie_gallery=tkinter.Label(root, text='è§†é¢‘æ’­æ”¾é€šé“')
    lab_movie_gallery.place(x=20,y=20,width=100,height=20)
    varRadio=tkinter.IntVar(value=1)
    Radiobutton1_movie_gallery=tkinter.Radiobutton(root,variable=varRadio,value=0,text='è§†é¢‘é€šé“1')
    Radiobutton2_movie_gallery = tkinter.Radiobutton(root, variable=varRadio, value=1, text='è§†é¢‘é€šé“2')
    Radiobutton1_movie_gallery.place(x=130,y=20,width=100,height=20)
    Radiobutton2_movie_gallery.place(x=250, y=20, width=100, height=20)

    varentry2=tkinter.StringVar(value='https://v.qq.com/x/cover/1o29ui77e85grdr/h0022ah1yrf.html')
    lab_movie_link = tkinter.Label(root, text='è§†é¢‘æ’­æ”¾é“¾æ¥')
    lab_movie_link.place(x=20, y=60, width=100, height=20)
    entry_movie_link = tkinter.Entry(root, textvariable=varentry2)
    entry_movie_link.place(x=130, y=60, width=300, height=20)
    button_movie_link=tkinter.Button(root,text='æ¸…ç©º',command=qk)
    button_movie_link.place(x=440,y=60,width=30,height=20)
    lab_remind = tkinter.Label(root, text='å°†è§†é¢‘é“¾æ¥å¤åˆ¶åˆ°æ¡†å†…ï¼Œç‚¹å‡»æ’­æ”¾VIPè§†é¢‘')
    lab_remind.place(x=50, y=90, width=400, height=20)
    varbutton=tkinter.StringVar
    button_movie= tkinter.Button(root, text='æ’­æ”¾VIPè§†é¢‘', command=Button)
    button_movie.place(x=140, y=120, width=200, height=60)

    button_movie1 = tkinter.Button(root, text='çˆ±å¥‡è‰º', command=openaqy)
    button_movie1.place(x=60, y=200, width=100, height=60)

    button_movie2 = tkinter.Button(root, text='è…¾è®¯è§†é¢‘', command=opentx)
    button_movie2.place(x=180, y=200, width=100, height=60)

    button_movie3 = tkinter.Button(root, text='ä¼˜é…·è§†é¢‘', command=openyq)
    button_movie3.place(x=300, y=200, width=100, height=60)

    root.mainloop()
çŸ¥ä¹
import requests

class Get_follow(object):
    def __init__(self,page):
        self.page=page
        self.url='https://www.zhihu.com/api/v4/members/mwycegusa/followers?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&amp;offset={}&amp;limit=20'.format(page*20)
        self.headers={
            'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'
        }
        self.cookie={
            'cookie':'_zap=02f27f98-87e4-43fd-a3c4-288621b56d09; d_c0="AMDlavhINg-PTn2F7IvBkJzHDhu7npy4o3k=|1554121045"; __gads=ID=057bb41b2faa002c:T=1554121053:S=ALNI_MYvPXKcplL1UDWgkALfG5CcunCbFQ; q_c1=04444d44ef5a4b87a6a22e9ed3ae687f|1557222293000|1554121051000; r_cap_id="ZmZmMjAyMDhkOTI2NGQ4ZmFkZTRmMjk2MWY5MjJjYjM=|1557222293|f0b285636da386a5a9425d18c252ac7019f22478"; cap_id="NTE2M2Y5Yjg0MDMxNDU0MmEyNTI5NGY4Mzk2MTU3MTA=|1557222293|06f8e2466a62fd12e4c0455bb190ca45b4aa7673"; l_cap_id="YzAwMGMyMzYyZTZiNDc4OWE4MGY5MzA1ODA4ODI4OTA=|1557222293|2b8fbb964c0596b102fe6c5385f52c8b49354717"; __utma=51854390.922669224.1557222296.1557222296.1557222296.1; __utmz=51854390.1557222296.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); __utmv=51854390.000--|3=entry_date=20190401=1; _xsrf=4LK6GhKpu3XhmKHkcAeQLS36FeuumzHq; capsion_ticket="2|1:0|10:1557642571|14:capsion_ticket|44:NjI5MzEwNDEzNGNjNGE2NGI4NGY4ZDczYTM2YjU5YjY=|28b611d285b2b29acb7f11f50c21b9d2333df95e3831b408ca5a2e750f86582a"; z_c0="2|1:0|10:1557642658|4:z_c0|92:Mi4xaUYwUkNRQUFBQUFBd09WcS1FZzJEeVlBQUFCZ0FsVk5vZ3ZGWFFCUFNsUzNpWkVjdFpKVDQ5NmpEOWxYT1pmQmVn|722efca13331bcfc016ea98db395ec0f59dbdde75b51285f0099f847fa794e21"; tst=f; tgw_l7_route=66cb16bc7f45da64562a077714739c11'
        }
    def get_html(self):
        response=requests.get(self.url,headers=self.headers,cookies=self.cookie)
        # data= response.json()['data']
        # print(data)
        list=[]
        for item in response.json()['data']:
            list.append(item['id'])
        print(list)


class Send_MSG(object):
        def __init__(self):
            self.url='https://www.zhihu.com/api/v4/messages'
            self.headers = {
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'
            }
            self.data={
                'content': "æœ¬æ¡å†…å®¹ç”±Python è„šæœ¬å‘é€ï¼Œå¦‚æœå¯ä»¥ï¼Œè¯·å…³æ³¨æˆ‘ï¼Œè°¢è°¢ï¼Œæ‰“æ‰°æ‰“æ‰°",
                'created_time': '1557720982',
                'has_read': 'false',
                'id': '1111243022582411264'
            }


        def Send(self):
            try:
                requests.post(self.url,json=self.data,headers=self.headers)
                print('å‘é€æˆåŠŸ')
            except:
                print('å‘é€å¤±è´¥')





if __name__ == '__main__':

    page=input('è¯·è¾“å…¥è¯¥ç”¨æˆ·ç²‰ä¸é¡µé¢æ•°ï¼š')
    for i in range(int(page)):
        a=Get_follow(i)
        a.get_html()
        msg_send=Send_MSG()
        msg_send.Send()
```
</dd></i></dd></div></div></div></div></module></stdin></censored...binary...data></censored...binary...data></head></html></head></html>
:ET